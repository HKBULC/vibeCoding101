Reference_Number,Reference_Text,Citation_Count,Context
1,"Ahonen H, Heinonen O, Klemettinen M, Verkamo AI (1 99 8) Applying data mining techniques for descriptive phrase extraction in digital document collections. In research and technology advances in digital libraries, 1 99 8. ADL 9 8. Proceedings. IEEE international forum on (pp. 2-1 1). IEEE ## 2. Alshawi H (1 99 2) The core language engine. MIT press ## 3. Alshemali B, Kalita J (2 02 0) Improving the reliability of deep neural networks in NLP: A review. Knowl- Based Syst 1 91:1 05 21 0 ## 4. Andreev ND (1 96 7) The intermediary language as the focal point of machine translation. In: Booth AD (ed) Machine translation. North Holland Publishing Company, Amsterdam, pp 3 – 2 7 ## 5. Androutsopoulos I, Paliouras G, Karkaletsis V, Sakkis G, Spyropoulos CD, Stamatopoulos P (2 00 0) Learning to filter spam e-mail: A comparison of a naive bayesian and a memory-based approach. ar Xiv preprint cs/0 00 90 09 ## 6. Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J (2 02 0) Artificial intelligence in public health: challenges and opportunities for public health made possible by advances in natural language processing. Can Commun Dis Rep 4 6(6):1 61 ## 7. Bahdanau D, Cho K, Bengio Y (2 01 5) Neural machine translation by jointly learning to align and translate. In ICLR 2 01 5 ## 8. Bangalore S, Rambow O, Whittaker S (2 00 0) Evaluation metrics for generation. In proceedings of the first international conference on natural language generation-volume 1 4 (pp. 1-8). Assoc Comput Linguist ## 9. Baud RH, Rassinoux AM, Scherrer JR (1 99 1) Knowledge representation of discharge summaries. In AIME 9 1 (pp. 1 73 – 1 82). Springer, Berlin Heidelberg",9,e ’ s Intelligent Text Analyzer) (Glasgow et al. (1 99 8) [ 4 8 ]) that extracts information from life insurance applications. Ahonen et al. (1 99 8) [ 1 ] suggested a mainstream framework for text mining that uses pragmatic and discourse level analyses of text . e) Summarization Overload of information
1,"Ahonen H, Heinonen O, Klemettinen M, Verkamo AI (1 99 8) Applying data mining techniques for descriptive phrase extraction in digital document collections. In research and technology advances in digital libraries, 1 99 8. ADL 9 8. Proceedings. IEEE international forum on (pp. 2-1 1). IEEE ## 2. Alshawi H (1 99 2) The core language engine. MIT press ## 3. Alshemali B, Kalita J (2 02 0) Improving the reliability of deep neural networks in NLP: A review. Knowl- Based Syst 1 91:1 05 21 0 ## 4. Andreev ND (1 96 7) The intermediary language as the focal point of machine translation. In: Booth AD (ed) Machine translation. North Holland Publishing Company, Amsterdam, pp 3 – 2 7 ## 5. Androutsopoulos I, Paliouras G, Karkaletsis V, Sakkis G, Spyropoulos CD, Stamatopoulos P (2 00 0) Learning to filter spam e-mail: A comparison of a naive bayesian and a memory-based approach. ar Xiv preprint cs/0 00 90 09 ## 6. Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J (2 02 0) Artificial intelligence in public health: challenges and opportunities for public health made possible by advances in natural language processing. Can Commun Dis Rep 4 6(6):1 61 ## 7. Bahdanau D, Cho K, Bengio Y (2 01 5) Neural machine translation by jointly learning to align and translate. In ICLR 2 01 5 ## 8. Bangalore S, Rambow O, Whittaker S (2 00 0) Evaluation metrics for generation. In proceedings of the first international conference on natural language generation-volume 1 4 (pp. 1-8). Assoc Comput Linguist ## 9. Baud RH, Rassinoux AM, Scherrer JR (1 99 1) Knowledge representation of discharge summaries. In AIME 9 1 (pp. 1 73 – 1 82). Springer, Berlin Heidelberg",9,"## 1 4. Berger AL, Della Pietra SA, Della Pietra VJ (1 99 6) A maximum entropy approach to natural language processing. Computational Linguistics 2 2(1):3 9 – 7 1 ## 1 5. Blanzieri E, Bryl A (2 00 8) A survey of learning-based techniques of email spam filtering. Artif Intell Rev 2 9(1):6 3 – 9 2 3 73"
1,"Ahonen H, Heinonen O, Klemettinen M, Verkamo AI (1 99 8) Applying data mining techniques for descriptive phrase extraction in digital document collections. In research and technology advances in digital libraries, 1 99 8. ADL 9 8. Proceedings. IEEE international forum on (pp. 2-1 1). IEEE ## 2. Alshawi H (1 99 2) The core language engine. MIT press ## 3. Alshemali B, Kalita J (2 02 0) Improving the reliability of deep neural networks in NLP: A review. Knowl- Based Syst 1 91:1 05 21 0 ## 4. Andreev ND (1 96 7) The intermediary language as the focal point of machine translation. In: Booth AD (ed) Machine translation. North Holland Publishing Company, Amsterdam, pp 3 – 2 7 ## 5. Androutsopoulos I, Paliouras G, Karkaletsis V, Sakkis G, Spyropoulos CD, Stamatopoulos P (2 00 0) Learning to filter spam e-mail: A comparison of a naive bayesian and a memory-based approach. ar Xiv preprint cs/0 00 90 09 ## 6. Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J (2 02 0) Artificial intelligence in public health: challenges and opportunities for public health made possible by advances in natural language processing. Can Commun Dis Rep 4 6(6):1 61 ## 7. Bahdanau D, Cho K, Bengio Y (2 01 5) Neural machine translation by jointly learning to align and translate. In ICLR 2 01 5 ## 8. Bangalore S, Rambow O, Whittaker S (2 00 0) Evaluation metrics for generation. In proceedings of the first international conference on natural language generation-volume 1 4 (pp. 1-8). Assoc Comput Linguist ## 9. Baud RH, Rassinoux AM, Scherrer JR (1 99 1) Knowledge representation of discharge summaries. In AIME 9 1 (pp. 1 73 – 1 82). Springer, Berlin Heidelberg",9,"Linguistics 2 2(1):3 9 – 7 1 ## 1 5. Blanzieri E, Bryl A (2 00 8) A survey of learning-based techniques of email spam filtering. Artif Intell Rev 2 9(1):6 3 – 9 2 3 73 8 Multimedia Tools and Applications (2 02 3) 8 2:3 71 3–3 74 4 <!-- Page 27 --> ## 1 6. Bondale N, Maloor P, Vaidyanathan A, Sengup"
1,"Ahonen H, Heinonen O, Klemettinen M, Verkamo AI (1 99 8) Applying data mining techniques for descriptive phrase extraction in digital document collections. In research and technology advances in digital libraries, 1 99 8. ADL 9 8. Proceedings. IEEE international forum on (pp. 2-1 1). IEEE ## 2. Alshawi H (1 99 2) The core language engine. MIT press ## 3. Alshemali B, Kalita J (2 02 0) Improving the reliability of deep neural networks in NLP: A review. Knowl- Based Syst 1 91:1 05 21 0 ## 4. Andreev ND (1 96 7) The intermediary language as the focal point of machine translation. In: Booth AD (ed) Machine translation. North Holland Publishing Company, Amsterdam, pp 3 – 2 7 ## 5. Androutsopoulos I, Paliouras G, Karkaletsis V, Sakkis G, Spyropoulos CD, Stamatopoulos P (2 00 0) Learning to filter spam e-mail: A comparison of a naive bayesian and a memory-based approach. ar Xiv preprint cs/0 00 90 09 ## 6. Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J (2 02 0) Artificial intelligence in public health: challenges and opportunities for public health made possible by advances in natural language processing. Can Commun Dis Rep 4 6(6):1 61 ## 7. Bahdanau D, Cho K, Bengio Y (2 01 5) Neural machine translation by jointly learning to align and translate. In ICLR 2 01 5 ## 8. Bangalore S, Rambow O, Whittaker S (2 00 0) Evaluation metrics for generation. In proceedings of the first international conference on natural language generation-volume 1 4 (pp. 1-8). Assoc Comput Linguist ## 9. Baud RH, Rassinoux AM, Scherrer JR (1 99 1) Knowledge representation of discharge summaries. In AIME 9 1 (pp. 1 73 – 1 82). Springer, Berlin Heidelberg",9,"9:1 61 3 – 1 61 9 ## 4 3. Fattah MA, Ren F (2 00 9) GA, MR, FFNN, PNN and GMM based models for automatic text summari- zation. Comput Speech Lang 2 3(1):1 26 – 1 44 3 73 9 Multimedia Tools and Applications (2 02 3) 8 2:3 71 3–3 74 4 <!-- Page 28 --> ## 4 4. Feldman S (1 99 9) NLP meets the jabberwo"
1,"Ahonen H, Heinonen O, Klemettinen M, Verkamo AI (1 99 8) Applying data mining techniques for descriptive phrase extraction in digital document collections. In research and technology advances in digital libraries, 1 99 8. ADL 9 8. Proceedings. IEEE international forum on (pp. 2-1 1). IEEE ## 2. Alshawi H (1 99 2) The core language engine. MIT press ## 3. Alshemali B, Kalita J (2 02 0) Improving the reliability of deep neural networks in NLP: A review. Knowl- Based Syst 1 91:1 05 21 0 ## 4. Andreev ND (1 96 7) The intermediary language as the focal point of machine translation. In: Booth AD (ed) Machine translation. North Holland Publishing Company, Amsterdam, pp 3 – 2 7 ## 5. Androutsopoulos I, Paliouras G, Karkaletsis V, Sakkis G, Spyropoulos CD, Stamatopoulos P (2 00 0) Learning to filter spam e-mail: A comparison of a naive bayesian and a memory-based approach. ar Xiv preprint cs/0 00 90 09 ## 6. Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J (2 02 0) Artificial intelligence in public health: challenges and opportunities for public health made possible by advances in natural language processing. Can Commun Dis Rep 4 6(6):1 61 ## 7. Bahdanau D, Cho K, Bengio Y (2 01 5) Neural machine translation by jointly learning to align and translate. In ICLR 2 01 5 ## 8. Bangalore S, Rambow O, Whittaker S (2 00 0) Evaluation metrics for generation. In proceedings of the first international conference on natural language generation-volume 1 4 (pp. 1-8). Assoc Comput Linguist ## 9. Baud RH, Rassinoux AM, Scherrer JR (1 99 1) Knowledge representation of discharge summaries. In AIME 9 1 (pp. 1 73 – 1 82). Springer, Berlin Heidelberg",9,"D, Ghemri L, Fisher D (1 99 8) MITA: an information-extraction approach to the analysis of free-form text in life insurance applications. AI Mag 1 9(1):5 9 ## 4 9. Goldberg Y (2 01 7) Neural network methods for natural language processing. Synthesis lectures on human language technologies 1 0(1):1 –"
1,"Ahonen H, Heinonen O, Klemettinen M, Verkamo AI (1 99 8) Applying data mining techniques for descriptive phrase extraction in digital document collections. In research and technology advances in digital libraries, 1 99 8. ADL 9 8. Proceedings. IEEE international forum on (pp. 2-1 1). IEEE ## 2. Alshawi H (1 99 2) The core language engine. MIT press ## 3. Alshemali B, Kalita J (2 02 0) Improving the reliability of deep neural networks in NLP: A review. Knowl- Based Syst 1 91:1 05 21 0 ## 4. Andreev ND (1 96 7) The intermediary language as the focal point of machine translation. In: Booth AD (ed) Machine translation. North Holland Publishing Company, Amsterdam, pp 3 – 2 7 ## 5. Androutsopoulos I, Paliouras G, Karkaletsis V, Sakkis G, Spyropoulos CD, Stamatopoulos P (2 00 0) Learning to filter spam e-mail: A comparison of a naive bayesian and a memory-based approach. ar Xiv preprint cs/0 00 90 09 ## 6. Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J (2 02 0) Artificial intelligence in public health: challenges and opportunities for public health made possible by advances in natural language processing. Can Commun Dis Rep 4 6(6):1 61 ## 7. Bahdanau D, Cho K, Bengio Y (2 01 5) Neural machine translation by jointly learning to align and translate. In ICLR 2 01 5 ## 8. Bangalore S, Rambow O, Whittaker S (2 00 0) Evaluation metrics for generation. In proceedings of the first international conference on natural language generation-volume 1 4 (pp. 1-8). Assoc Comput Linguist ## 9. Baud RH, Rassinoux AM, Scherrer JR (1 99 1) Knowledge representation of discharge summaries. In AIME 9 1 (pp. 1 73 – 1 82). Springer, Berlin Heidelberg",9,"1 9(1):5 9 ## 4 9. Goldberg Y (2 01 7) Neural network methods for natural language processing. Synthesis lectures on human language technologies 1 0(1):1 – 3 09 ## 5 0. Gong Y, Liu X (2 00 1) Generic text summarization using relevance measure and latent semantic analysis. In proceedings of the 2 4 t"
1,"Ahonen H, Heinonen O, Klemettinen M, Verkamo AI (1 99 8) Applying data mining techniques for descriptive phrase extraction in digital document collections. In research and technology advances in digital libraries, 1 99 8. ADL 9 8. Proceedings. IEEE international forum on (pp. 2-1 1). IEEE ## 2. Alshawi H (1 99 2) The core language engine. MIT press ## 3. Alshemali B, Kalita J (2 02 0) Improving the reliability of deep neural networks in NLP: A review. Knowl- Based Syst 1 91:1 05 21 0 ## 4. Andreev ND (1 96 7) The intermediary language as the focal point of machine translation. In: Booth AD (ed) Machine translation. North Holland Publishing Company, Amsterdam, pp 3 – 2 7 ## 5. Androutsopoulos I, Paliouras G, Karkaletsis V, Sakkis G, Spyropoulos CD, Stamatopoulos P (2 00 0) Learning to filter spam e-mail: A comparison of a naive bayesian and a memory-based approach. ar Xiv preprint cs/0 00 90 09 ## 6. Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J (2 02 0) Artificial intelligence in public health: challenges and opportunities for public health made possible by advances in natural language processing. Can Commun Dis Rep 4 6(6):1 61 ## 7. Bahdanau D, Cho K, Bengio Y (2 01 5) Neural machine translation by jointly learning to align and translate. In ICLR 2 01 5 ## 8. Bangalore S, Rambow O, Whittaker S (2 00 0) Evaluation metrics for generation. In proceedings of the first international conference on natural language generation-volume 1 4 (pp. 1-8). Assoc Comput Linguist ## 9. Baud RH, Rassinoux AM, Scherrer JR (1 99 1) Knowledge representation of discharge summaries. In AIME 9 1 (pp. 1 73 – 1 82). Springer, Berlin Heidelberg",9,"7 – 2 37 ## 1 00. Palmer M, Gildea D, Kingsbury P (2 00 5) The proposition bank: an annotated corpus of semantic roles. Computational linguistics 3 1(1):7 1 – 1 06 3 74 1 Multimedia Tools and Applications (2 02 3) 8 2:3 71 3–3 74 4 <!-- Page 30 --> ## 1 01. Papineni K, Roukos S, Ward T, Zhu WJ (2 0"
1,"Ahonen H, Heinonen O, Klemettinen M, Verkamo AI (1 99 8) Applying data mining techniques for descriptive phrase extraction in digital document collections. In research and technology advances in digital libraries, 1 99 8. ADL 9 8. Proceedings. IEEE international forum on (pp. 2-1 1). IEEE ## 2. Alshawi H (1 99 2) The core language engine. MIT press ## 3. Alshemali B, Kalita J (2 02 0) Improving the reliability of deep neural networks in NLP: A review. Knowl- Based Syst 1 91:1 05 21 0 ## 4. Andreev ND (1 96 7) The intermediary language as the focal point of machine translation. In: Booth AD (ed) Machine translation. North Holland Publishing Company, Amsterdam, pp 3 – 2 7 ## 5. Androutsopoulos I, Paliouras G, Karkaletsis V, Sakkis G, Spyropoulos CD, Stamatopoulos P (2 00 0) Learning to filter spam e-mail: A comparison of a naive bayesian and a memory-based approach. ar Xiv preprint cs/0 00 90 09 ## 6. Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J (2 02 0) Artificial intelligence in public health: challenges and opportunities for public health made possible by advances in natural language processing. Can Commun Dis Rep 4 6(6):1 61 ## 7. Bahdanau D, Cho K, Bengio Y (2 01 5) Neural machine translation by jointly learning to align and translate. In ICLR 2 01 5 ## 8. Bangalore S, Rambow O, Whittaker S (2 00 0) Evaluation metrics for generation. In proceedings of the first international conference on natural language generation-volume 1 4 (pp. 1-8). Assoc Comput Linguist ## 9. Baud RH, Rassinoux AM, Scherrer JR (1 99 1) Knowledge representation of discharge summaries. In AIME 9 1 (pp. 1 73 – 1 82). Springer, Berlin Heidelberg",9,"mer EL, Eddy SR, Birney E, Bateman A, Durbin R (1 99 8) Pfam: multiple sequence alignments and HMM-profiles of protein domains. Nucleic Acids Res 2 6(1):3 20 – 3 22 ## 1 29. Srihari S (2 01 0) Machine Learning: Generative and Discriminative Models. http://www.cedar.buffalo.edu/ wsrihari/CSE 57 4/Disc"
1,"Ahonen H, Heinonen O, Klemettinen M, Verkamo AI (1 99 8) Applying data mining techniques for descriptive phrase extraction in digital document collections. In research and technology advances in digital libraries, 1 99 8. ADL 9 8. Proceedings. IEEE international forum on (pp. 2-1 1). IEEE ## 2. Alshawi H (1 99 2) The core language engine. MIT press ## 3. Alshemali B, Kalita J (2 02 0) Improving the reliability of deep neural networks in NLP: A review. Knowl- Based Syst 1 91:1 05 21 0 ## 4. Andreev ND (1 96 7) The intermediary language as the focal point of machine translation. In: Booth AD (ed) Machine translation. North Holland Publishing Company, Amsterdam, pp 3 – 2 7 ## 5. Androutsopoulos I, Paliouras G, Karkaletsis V, Sakkis G, Spyropoulos CD, Stamatopoulos P (2 00 0) Learning to filter spam e-mail: A comparison of a naive bayesian and a memory-based approach. ar Xiv preprint cs/0 00 90 09 ## 6. Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J (2 02 0) Artificial intelligence in public health: challenges and opportunities for public health made possible by advances in natural language processing. Can Commun Dis Rep 4 6(6):1 61 ## 7. Bahdanau D, Cho K, Bengio Y (2 01 5) Neural machine translation by jointly learning to align and translate. In ICLR 2 01 5 ## 8. Bangalore S, Rambow O, Whittaker S (2 00 0) Evaluation metrics for generation. In proceedings of the first international conference on natural language generation-volume 1 4 (pp. 1-8). Assoc Comput Linguist ## 9. Baud RH, Rassinoux AM, Scherrer JR (1 99 1) Knowledge representation of discharge summaries. In AIME 9 1 (pp. 1 73 – 1 82). Springer, Berlin Heidelberg",9,"cht) ## 1 44. Wan X (2 00 8) Using only cross-document relationships for both generic and topic-focused multi-docu- ment summarizations. Inf Retr 1 1(1):2 5 – 4 9 ## 1 45. Wang W, Gang J, 2 01 8 Application of convolutional neural network in natural language processing. In 2 01 8 international confer"
10,"Baud RH, Rassinoux AM, Scherrer JR (1 99 2) Natural language processing and semantical representation of medical texts. Methods Inf Med 3 1(2):1 17 – 1 25",2,"ava RK, Koutník J, Steunebrink BR, Schmidhuber J (2 01 6) LSTM: A search space odyssey. IEEE transactions on neural networks and learning systems 2 8(1 0):2 22 2 – 2 23 2 ## 5 3. Grishman R, Sager N, Raze C, Bookchin B (1 97 3) The linguistic string parser. In proceedings of the June 4-8, 1 97 3, natio"
10,"Baud RH, Rassinoux AM, Scherrer JR (1 99 2) Natural language processing and semantical representation of medical texts. Methods Inf Med 3 1(2):1 17 – 1 25",2,"iedhammer K, Favre B, Hakkani-Tür D (2 01 0) Long story short – global unsupervised models for keyphrase based meeting summarization. Speech Comm 5 2(1 0):8 01 – 8 15 ## 1 11. Ritter A, Clark S, Etzioni O (2 01 1) Named entity recognition in tweets: an experimental study. In proceedings of the conferen"
11,"Baud RH, Alpay L, Lovis C (1 99 4) Let ’ s meet the users with natural language understanding. Knowledge and Decisions in Health Telematics: The Next Decade 1 2:1 03",1,"01 9) Unsupervised cross-media retrieval using domain adaptation with scene graph. IEEE Transactions on Circuits and Systems for Video Technology 3 0(1 1):4 36 8 – 4 37 9 ## 1 03. Porter MF (1 98 0) An algorithm for suffix stripping. Program 1 4(3):1 30 – 1 37 ## 1 04. Rae JW, Potapenko A, Jayakumar SM"
12,"Bengio Y, Ducharme R, Vincent P (2 00 1) A neural probabilistic language model. Proceedings of NIPS",1,"arly 2 00 0, neural language modeling in which the probability of occurring of next word (token) is determined given n previous words. Bendigo et al. [ 1 2 ] proposed the concept of feed forward neural network and lookup table which represents the n previous words in sequence. Collobert et al. [ 2 9 ] prop"
13,"Benson E, Haghighi A, Barzilay R (2 01 1) Event discovery in social media feeds. In proceedings of the 4 9 th annual meeting of the Association for Computational Linguistics: human language technologies- volume 1 (pp. 3 89-3 98). Assoc Comput Linguist",1,"a given verb, and finally classifying these nodes to compute the corresponding SRL tags. Event discovery in social media feeds (Benson et al.,2 01 1) [ 1 3 ], using a graphical model to analyze any social media feeds to determine whether it contains the name of a person or name of a venue, place, time etc."
14,"Berger AL, Della Pietra SA, Della Pietra VJ (1 99 6) A maximum entropy approach to natural language processing. Computational Linguistics 2 2(1):3 9 – 7 1",1,"vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding method (T. Xia, 2 02 0) [ 1 53 ], sometimes combining different learners (Sakkis et al., 2 00 1) [ 1 16 ]. Using the"
15,"Blanzieri E, Bryl A (2 00 8) A survey of learning-based techniques of email spam filtering. Artif Intell Rev 2 9(1):6 3 – 9 2 3 73 8 Multimedia Tools and Applications (2 02 3) 8 2:3 71 3–3 74 4 <!-- Page 27 -->",1,"ment. Most text categorization approaches to anti-spam Email filtering have used multi variate Bernoulli model (Androutsopoulos et al., 2 00 0) [ 5 ] [ 1 5 ]. d) Information Extraction Information extraction is concerned with identifying phrases of interest of textual data. For many applications, extractin"
16,"Bondale N, Maloor P, Vaidyanathan A, Sengupta S, Rao PV (1 99 9) Extraction of information from open- ended questionnaires using natural language processing techniques. Computer Science and Informatics 2 9(2):1 5 – 2 2",1,"on to discourse analysis at the level of the complete document. An application of the Blank Slate Language Processor (BSLP) ( Bondale et al., 1 99 9) [ 1 6 ] approach for the analysis of a real-life natural language corpus that consists of responses to open-ended questionnaires in the field of advertising."
18,"Briscoe EJ, Grover C, Boguraev B, Carroll J (1 98 7) A formalism and environment for the development of a large grammar of English. IJCAI 8 7:7 03 – 7 08",1,"mmunities. Practical resources, grammars, and tools and parsers became available (for example: Alvey Natural Language Tools) (Briscoe et al., 1 98 7) [ 1 8 ]. The (D)ARPA speech recognition and message understanding (information extraction) conferences were not only for the tasks they addressed 3 72 0 Mult"
19,"Carreras X, Marquez L (2 00 1) Boosting trees for anti-spam email filtering. ar Xiv preprint cs/0 10 90 15",1,"sed Learning (Sakkiset al.,2 00 0 b) [ 1 17 ], Support vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding method (T. Xia, 2 02 0) [ 1 53 ], sometimes combining differen"
20,"Chalkidis I, Fergadiotis M, Malakasiotis P, Aletras N, Androutsopoulos I (2 02 0) LEGAL-BERT: the muppets straight out of law school. ar Xiv preprint ar Xiv:2 01 0.0 25 59",1,"9 0 ] used the BERT model to analyze the tweets on covid-1 9 content. The use of the BERT model in the legal domain was explored by Chalkidis et al. [ 2 0 ]. Since BERT considers up to 5 12 tokens, this is the reason if there is a long text sequence that must be divided into multiple short text sequences"
23,"Chomsky N (1 96 5) Aspects of the theory of syntax. MIT Press, Cambridge, Massachusetts",1,"d syntactic theories, marked a unique position in the field of theoretical linguistics because he revolutionized the area of syntax (Chomsky, 1 96 5) [ 2 3 ]. Further, Natural Language Genera- tion (NLG) is the process of producing phrases, sentences and paragraphs that are meaningful from an internal repr"
24,Choudhary N (2 02 1) LDC-IL: the Indian repository of resources for language technology. Lang Resources & Evaluation 5 5:8 55 – 8 67. https://doi.org/1 0.1 00 7/s 10 57 9-0 20-0 95 23-3,1,chnology Development Programme for Indian Languages (TDIL) launched its own data distribution portal ( www. tdil-dc.in ) which has cataloged datasets [ 2 4 ]. 3. Machine Translation: The task of converting the text of one natural language into another language while keeping the sense of the input text is k
26,"Chung J, Gulcehre C, Cho K, Bengio Y, (2 01 4) Empirical evaluation of gated recurrent neural networks on sequence modeling. ar Xiv preprint ar Xiv:1 41 2.3 55 5",1,"also led to a slightly simpler variant, called the gated recurrent unit (GRU), which has shown better results than standard LSTMs in many tasks [ 2 2 , 2 6 ]. Attention mechanisms [ 7 ] which suggest a network to learn what to pay attention to in accordance with the current hidden state and annotation toge"
27,"Cohen WW (1 99 6) Learning rules that classify e-mail. In AAAI spring symposium on machine learning in information access (Vol. 1 8, p. 2 5)",1,"n recent times, various machine learning techniques have been applied to text categorization or Anti-Spam Filtering like Rule Learning (Cohen 1 99 6) [ 2 7 ], Naïve Bayes (Sahami et al., 1 99 8; Androutsopoulos et al., 2 00 0; Rennie.,2 00 0) [ 5 , 1 09 , 1 15 ],Memory based Learning (Sakkiset al.,2 00 0 b"
28,"Cohen PR, Morgan J, Ramsay AM (2 00 2) Intention in communication, Am J Psychol 1 04(4)",1,"inchor,1 99 3) [ 1 31 , 1 57 ]. Work on user modeling (Wahlster and Kobsa, 1 98 9) [ 1 42 ] was one strand in a research paper. Cohen et al. (2 00 2) [ 2 8 ] had put forwarded a first approximation of a compositional theory of tune interpretation, together with phonological assumptions on which it is based"
29,"Collobert R, Weston J (2 00 8) A unified architecture for natural language processing. In proceedings of the 2 5 th international conference on machine learning (pp. 1 60 – 1 67)",1,"al. [ 1 2 ] proposed the concept of feed forward neural network and lookup table which represents the n previous words in sequence. Collobert et al. [ 2 9 ] proposed the application of multitask learning in the field of NLP, where two convolutional models with max pooling were used to perform parts-of-spe"
30,"Dai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov R, (2 01 9) Transformer-xl: attentive language models beyond a fixed-length context. ar Xiv preprint ar Xiv:1 90 1.0 28 60",1,learning longer-term dependency but are limited by a fixed-length context in the setting of language modeling. In this direction recently Dai et al. [ 3 0 ] proposed a novel neural architecture Transformer-XL (XL as extra- long) which enables learning dependencies beyond a fixed length of words. Further t
31,"Davis E, Marcus G (2 01 5) Commonsense reasoning and commonsense knowledge in artificial intelli- gence. Commun ACM 5 8(9):9 2 – 1 03",1,"view work by Alshemali and Kalita (2 02 0) [ 3 ], and references cited there in. In the recent past, models dealing with Visual Commonsense Reasoning [ 3 1 ] and NLP have also been getting attention of the several researchers and seems a promising and challenging area to work upon. These models try to extr"
33,"Devlin J, Chang MW, Lee K, Toutanova K, (2 01 8) Bert: pre-training of deep bidirectional transformers for language understanding. ar Xiv preprint ar Xiv:1 81 0.0 48 05",1,"3) 8 2:3 71 3–3 74 4 <!-- Page 11 --> relevant references cited therein. The use of BERT (Bidirectional Encoder Representations from Transformers) [ 3 3 ] model and successive models have also played an important role for NLP. Many researchers worked on NLP, building tools and systems which makes NLP wh"
34,"Diab M, Hacioglu K, Jurafsky D (2 00 4) Automatic tagging of Arabic text: From raw text to base phrase chunks. In Proceedings of HLT-NAACL 2 00 4: Short papers (pp. 1 49 – 1 52). Assoc Computat Linguist",1,"each word using suffix stripping algorithm, wherein the longest suffix is searched from the suffix table and tags are assigned. Diab et al. (2 00 4) [ 3 4 ] used supervised machine learning approach and adopted Support Vector Machines (SVMs) which were trained on the Arabic Treebank to automati- cally tok"
35,Doddington G (2 00 2) Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In proceedings of the second international conference on human language technology research (pp. 1 38-1 45). Morgan Kaufmann publishers Inc,1,"8 ], multi-reference word error rate (Nießen et al., 2 00 0) [ 9 5 ], BLEU score (Papineni et al., 2 00 2) [ 1 01 ], NIST score (Doddington, 2 00 2) [ 3 5 ] All these criteria try to approximate human assessment and often achieve an astonishing degree of correlation to human subjective evaluation of fluen"
36,"Drucker H, Wu D, Vapnik VN (1 99 9) Support vector machines for spam categorization. IEEE Trans Neural Netw 1 0(5):1 04 8 – 1 05 4",1,", 2 00 0; Rennie.,2 00 0) [ 5 , 1 09 , 1 15 ],Memory based Learning (Sakkiset al.,2 00 0 b) [ 1 17 ], Support vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding method"
37,"Dunlavy DM, O ’ Leary DP, Conroy JM, Schlesinger JD (2 00 7) QCS: A system for querying, clustering and summarizing documents. Inf Process Manag 4 3(6):1 58 8 – 1 60 5",1,59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani and Maybury 1 99 9 [ 7 4 ]; Fat
38,Elkan C (2 00 8) Log-Linear Models and Conditional Random Fields. http://cseweb.ucsd.edu/welkan/2 50 B/ cikmtutorial.pdf accessed 2 8 Jun 2 01 7.,1,"between languages. Whereas generative models can become troublesome when many features are used and discriminative models allow use of more features [ 3 8 ]. Few of the examples of discriminative methods are Logistic regression and conditional random fields (CRFs), generative methods are Naive Bayes class"
40,"Europarl: A Parallel Corpus for Statistical Machine Translation (2 00 5) Philipp Koehn , MT Summit 2 00 5",1,"of many sentences. b) The Europarl parallel corpus is derived from the European Parliament ’ s proceedings. It is available in 2 1 European languages [ 4 0 ]. c) WMT 14 provides machine translation pairs for English-German and English-French. Separately, these datasets comprise 4.5 million and 3 5 million"
41,"Fan Y, Tian F, Xia Y, Qin T, Li XY, Liu TY (2 02 0) Searching better architectures for neural machine translation. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2 8:1 57 4 – 1 58 5",1,"dy (BLEU) scores compared to various neural machine translation systems. It outperformed the commonly used MT system on a WMT 1 4 dataset. Fan et al. [ 4 1 ] introduced a gradient-based neural architecture search algorithm that automatically finds architecture with better performance than a transformer, co"
42,"Fang H, Lu W, Wu F, Zhang Y, Shang X, Shao J, Zhuang Y (2 01 5) Topic aspect-oriented summarization via group selection. Neurocomputing 1 49:1 61 3 – 1 61 9",1,resent entity. Various topics can have various aspects and various preferences of features are used to represent various aspects. (Fang et al. 2 01 5 [ 4 2 ]) f) Dialogue System Dialogue systems are very prominent in real world applications ranging from providing support to performing a particular action.
43,"Fattah MA, Ren F (2 00 9) GA, MR, FFNN, PNN and GMM based models for automatic text summari- zation. Comput Speech Lang 2 3(1):1 26 – 1 44 3 73 9 Multimedia Tools and Applications (2 02 3) 8 2:3 71 3–3 74 4 <!-- Page 28 -->",2,he two important categories are single document summarization and multi document summari- zation (Zajic et al. 2 00 8 [ 1 59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyan
43,"Fattah MA, Ren F (2 00 9) GA, MR, FFNN, PNN and GMM based models for automatic text summari- zation. Comput Speech Lang 2 3(1):1 26 – 1 44 3 73 9 Multimedia Tools and Applications (2 02 3) 8 2:3 71 3–3 74 4 <!-- Page 28 -->",2,]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani and Maybury 1 99 9 [ 7 4 ]; Fattah and Ren 2 00 9 [ 4 3 ]; Riedhammer et al. 2 01 0 [ 1 10 ]). Training data is required in a supervised system for selecting relevant material from the documents. Large amoun
44,Feldman S (1 99 9) NLP meets the jabberwocky: natural language processing in information retrieval. Online-Weston Then Wilton 2 3:6 2 – 7 3,1,"hna ” . That is why, to get the proper meaning of the sentence, the appropriate interpre- tation is considered by looking at the rest of the sentence [ 4 4 ]. f) Discourse While syntax and semantics level deal with sentence-length units, the discourse level of NLP deals with more than one sentence. It deal"
45,"Friedman C, Cimino JJ, Johnson SB (1 99 3) A conceptual model for clinical radiology reports. In proceedings of the annual symposium on computer application in medical care (p. 8 29). Am Med Inform Assoc",1,and Encoding System) that identifies clinical information in narrative reports and transforms the textual information into structured representation [ 4 5 ]. ### 3.3 NLP in talk We next discuss some of the recent NLP projects implemented by various companies: a) ACE Powered GDPR Robot Launched by RAVN Sy
47,"Ghosh S, Vinyals O, Strope B, Roy S, Dean T, Heck L (2 01 6) Contextual lstm (clstm) models for large scale nlp tasks. ar Xiv preprint ar Xiv:1 60 2.0 62 91",1,"tasks such 3 73 4 Multimedia Tools and Applications (2 02 3) 8 2:3 71 3–3 74 4 <!-- Page 23 --> as word prediction, and sentence topic prediction. [ 4 7 ] In order to observe the word arrange- ment in forward and backward direction, bi-directional LSTM is explored by researchers [ 5 9 ]. In case of mach"
48,"Glasgow B, Mandell A, Binney D, Ghemri L, Fisher D (1 99 8) MITA: an information-extraction approach to the analysis of free-form text in life insurance applications. AI Mag 1 9(1):5 9",1,open-ended questionnaires in the field of advertising. There is a system called MITA (Metlife ’ s Intelligent Text Analyzer) (Glasgow et al. (1 99 8) [ 4 8 ]) that extracts information from life insurance applications. Ahonen et al. (1 99 8) [ 1 ] suggested a mainstream framework for text mining that uses
49,Goldberg Y (2 01 7) Neural network methods for natural language processing. Synthesis lectures on human language technologies 1 0(1):1 – 3 09,1,"rieval, text summarization, text classification, machine transla- tion, sentiment analysis and speech recognition. Initially focus was on feedforward [ 4 9 ] and CNN (convolutional neural network) architecture [ 6 9 ] but later researchers adopted recurrent neural networks to capture the context of a word"
50,"Gong Y, Liu X (2 00 1) Generic text summarization using relevance measure and latent semantic analysis. In proceedings of the 2 4 th annual international ACM SIGIR conference on research and development in information retrieval (pp. 1 9-2 5). ACM",1,ation (Zajic et al. 2 00 8 [ 1 59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani
51,"Green Jr, BF, Wolf AK, Chomsky C, Laughery K (1 96 1) Baseball: an automatic question-answerer. In papers presented at the may 9-1 1, 1 96 1, western joint IRE-AIEE-ACM computer conference (pp. 2 19- 2 24). ACM",1,"linguistic studies had also started. As early as 1 96 0, signature work influenced by AI began, with the BASEBALL Q-A systems (Green et al., 1 96 1) [ 5 1 ]. LUNAR (Woods,1 97 8) [ 1 52 ] and Winograd SHRDLU were natural successors of these systems, but they were seen as stepped-up sophistication, in term"
54,"Hayes PJ (1 99 2) Intelligent high-volume text processing using shallow, domain-specific techniques. Text- based intelligent systems: current research and practice in information extraction and retrieval, 2 27-2 42.",2,"market data, newswires etc. and assign them to predefined categories or indices. For example, The Carnegie Group ’ s Construe system (Hayes, 1 99 1) [ 5 4 ], inputs Reuters articles and saves much time by doing the work that is to be done by staff or human indexers. Some companies have been using categori"
54,"Hayes PJ (1 99 2) Intelligent high-volume text processing using shallow, domain-specific techniques. Text- based intelligent systems: current research and practice in information extraction and retrieval, 2 27-2 42.",2,"ng to some pre- defined categories etc. For example, CONSTRUE, it was developed for Reuters, that is used in classifying news stories (Hayes, 1 99 2) [ 5 4 ]. It has been suggested that many IE systems can successfully extract terms from documents, acquiring relations between the terms is still a difficult"
55,"Hendrix GG, Sacerdoti ED, Sagalowicz D, Slocum J (1 97 8) Developing a natural language interface to complex data. ACM Transactions on Database Systems (TODS) 3(2):1 05 – 1 47 5 6. ""Here ’ s Why Natural Language Processing is the Future of BI (2 01 7) "" Smart Data Collective. N.p., n.d. Web. 1 9",1,"ject (Lea, 1 98 0) and other in some major system developments projects building database front ends. The front-end projects (Hendrix et al., 1 97 8) [ 5 5 ] were intended to go beyond LUNAR in interfacing the large databases. In early 1 98 0 s computational grammar theory became a very active area of rese"
56,"""Here ’ s Why Natural Language Processing is the Future of BI (2 01 7) "" Smart Data Collective. N.p., n.d. Web. 1 9",1,alcontent.com/stocks/news/read/3 38 88 79 5/RAVN_Systems_ Launch_the_ACE_Powered_GDPR_Robot b) Eno A Natural Language Chatbot Launched by Capital One [ 5 6 ] Capital One announces a chatbot for customers called Eno. Eno is a natural language chatbot that people socialize through texting. Capital One claims
58,"Hochreiter S, Schmidhuber J (1 99 7) Long short-term memory. Neural Comput 9(8):1 73 5 – 1 78 0",1,"n the cases where only the desired important information needs to be retained for a much longer time discarding the irrelevant information, see [ 5 2 , 5 8 ]. Further development in the LSTM has also led to a slightly simpler variant, called the gated recurrent unit (GRU), which has shown better results th"
59,"Huang Z, Xu W, Yu K (2 01 5) Bidirectional LSTM-CRF models for sequence tagging. ar Xiv preprint ar Xiv:1 50 8.0 19 91",1,"opic prediction. [ 4 7 ] In order to observe the word arrange- ment in forward and backward direction, bi-directional LSTM is explored by researchers [ 5 9 ]. In case of machine translation, encoder-decoder architecture is used where dimensionality of input and output vector is not known. Neural networks c"
60,"Hutchins WJ (1 98 6) Machine translation: past, present, future (p. 6 6). Ellis Horwood, Chichester",1,"C report, which concluded that MT is going nowhere. But later, some MT production systems were providing output to their customers (Hutchins, 1 98 6) [ 6 0 ]. By this time, work on the use of computers for literary and linguistic studies had also started. As early as 1 96 0, signature work influenced by AI"
61,"Jurafsky D, Martin J (2 00 8) H. Speech and language processing. 2 nd edn. Prentice-Hall, Englewood Cliffs, NJ",1,t is also explored in unusual areas like segmentation for infant learning and identifying documents for opinions and facts. Anggraeni et al. (2 01 9) [ 6 1 ] used ML and AI to create a question-and-answer system for retrieving information about hearing loss. They developed I-Chat Bot which understands the
62,"Kamp H, Reyle U (1 99 3) Tense and aspect. In from discourse to logic (pp. 4 83-6 89). Springer Netherlands",1,"eral purpose sentence processors like SRI ’ s Core Language Engine (Alshawi,1 99 2) [ 2 ] and Discourse Representation Theory (Kamp and Reyle,1 99 3) [ 6 2 ] offered a means of tackling more extended discourse within the grammatico-logical framework. This period was one of the growing communities. Practica"
63,"Kang Y, Cai Z, Tan CW, Huang Q, Liu H (2 02 0) Natural language processing (NLP) in management research: A literature review. Journal of Management Analytics 7(2):1 39 – 1 72",1,"nd challenges, Baclic et.al. (2 02 0) [ 6 ] and Wong et al. (2 01 8) [ 1 51 ] for challenges and opportunities in public health, Kang et.al. (2 02 0) [ 6 3 ] for detailed literature survey and technological challenges relevant to management research and NLP, and a recent review work by Alshemali and Kalita"
64,Kim Y. (2 01 4) Convolutional neural networks for sentence classification. ar Xiv preprint ar Xiv:1 40 8.5 88 2,1,"eacher who responds by offering brief extracts from the text. The neural learning models are overtaking traditional models for NLP [ 6 4 , 1 27 ]. In [ 6 4 ], authors used CNN (Convolutional Neural Network) model for sentiment analysis of movie reviews and achieved 8 1.5% accuracy. The results illustrate t"
66,"Lass R (1 99 8) Phonology: An Introduction to Basic Concepts. Cambridge, UK; New York; Melbourne, Australia: Cambridge University Press. p. 1. ISBN 9 78 – 0 – 5 21-2 37 28-4. Retrieved 8 January 2 01 1 Paperback ISBN 0 – 5 21 – 2 81 83-0",1,"pplications (2 02 3) 8 2:3 71 3–3 74 4 <!-- Page 4 --> Phonology is “ the study of sound pertaining to the system of language ” whereas Lass 19 98 [ 6 6 ]wrote that phonology refers broadly with the sounds of language, concerned with sub- discipline of linguistics, behavior and organization of sounds. P"
67,"Lewis DD (1 99 8) Naive (Bayes) at forty: The independence assumption in information retrieval. In European conference on machine learning (pp. 4 – 1 5). Springer, Berlin Heidelberg",1,"learned from training data rather than making by hand. The naïve bayes is preferred because of its performance despite its simplicity (Lewis, 1 99 8) [ 6 7 ] In Text Categorization two types of models have been used (Mc Callum and Nigam, 1 99 8) [ 7 7 ]. Both modules assume that a fixed vocabulary is prese"
68,Liddy ED (2 00 1). Natural language processing,3,"e ” or “ script ” . This level of processing also incorporates the semantic disambiguation of words with multiple senses (Elizabeth D. Liddy, 2 00 1) [ 6 8 ]. For example, the word “ bark ” as a noun can mean either as a sound that a dog makes or outer covering of the tree. The semantic level examines word"
68,Liddy ED (2 00 1). Natural language processing,3,"that convey meaning by interpreting the relations between sentences and uncovering linguistic structures from texts at several levels (Liddy,2 00 1) [ 6 8 ]. The two of the most common levels are: Anaphora Resolution an d Coreference Resolution. Anaphora resolution 3 71 7 Multimedia Tools and Applications"
68,Liddy ED (2 00 1). Natural language processing,3,"els of language. Habitable dialogue systems offer potential for fully automated dialog systems by utilizing all levels of a language. (Liddy, 2 00 1) [ 6 8 ].This leads to producing systems that can enable robots to interact with humans in natural languages such as Google ’ s assistant, Windows Cortana, Ap"
69,"Lopez MM, Kalita J (2 01 7) Deep learning applied to NLP. ar Xiv preprint ar Xiv:1 70 3.0 30 91",1,"nsla- tion, sentiment analysis and speech recognition. Initially focus was on feedforward [ 4 9 ] and CNN (convolutional neural network) architecture [ 6 9 ] but later researchers adopted recurrent neural networks to capture the context of a word with respect to surrounding words of a sentence. LSTM (Long"
70,"Luong MT, Sutskever I, Le Q V, Vinyals O, Zaremba W (2 01 4) Addressing the rare word problem in neural machine translation. ar Xiv preprint ar Xiv:1 41 0.8 20 6",2,"ks like Sentence Classification [ 1 27 ], Sentiment Analysis [ 1 35 ], Text Classification [ 1 18 ], Text Summarization [ 1 58 ], Machine Translation [ 7 0 ] and Answer Relations [ 1 50 ] . An article by Newatia (2 01 9) [ 9 3 ] illustrates the general architecture behind any CNN model, and how it can be u"
70,"Luong MT, Sutskever I, Le Q V, Vinyals O, Zaremba W (2 01 4) Addressing the rare word problem in neural machine translation. ar Xiv preprint ar Xiv:1 41 0.8 20 6",2,"using Penn Treebank dataset and word-level modeling using Wiki Text-1 03. In both cases, their model outshined the state-of-art methods. Luong et al. [ 7 0 ] used neural machine translation on the WMT 14 dataset and performed translation of English text to French text. The model demonstrated a significant"
73,"Maas A, Daly RE, Pham PT, Huang D, Ng AY, Potts C (2 01 1) Learning word vectors for sentiment analysis. In proceedings of the 4 9 th annual meeting of the association for computational linguistics: human language technologies (pp. 1 42-1 50)",1,"alysis, this dataset offers thousands of movie reviews split into training and test datasets. This dataset was introduced in by Mass et al. in 2 01 1 [ 7 3 ]. e) G.Rama Rohit Reddy of the Language Technologies Research Centre, KCIS, IIIT Hyderabad, generated the corpus “ Sentiraama. ” The corpus is divided"
74,"Mani I, Maybury MT (eds) (1 99 9) Advances in automatic text summarization, vol 2 93. MIT press, Cambridge, MA",2,"nd Schuetze,1 99 9) [ 7 5 ], because this not only involves data analysts. Information extraction and automatic summarizing (Mani and Maybury,1 99 9) [ 7 4 ] was also a point of focus. Next, we present a walkthrough of the developments from the early 2 00 0. ### 3.1 A walkthrough of recent developments in"
74,"Mani I, Maybury MT (eds) (1 99 9) Advances in automatic text summarization, vol 2 93. MIT press, Cambridge, MA",2,00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani and Maybury 1 99 9 [ 7 4 ]; Fattah and Ren 2 00 9 [ 4 3 ]; Riedhammer et al. 2 01 0 [ 1 10 ]). Training data is required in a supervised system for selecting relevant material
75,"Manning CD, Schütze H (1 99 9) Foundations of statistical natural language processing, vol 9 99. MIT press, Cambridge",1,"LP, the work on the lexicon, also pointed in this direction. Statistical language processing was a major thing in 9 0 s (Manning and Schuetze,1 99 9) [ 7 5 ], because this not only involves data analysts. Information extraction and automatic summarizing (Mani and Maybury,1 99 9) [ 7 4 ] was also a point of"
76,"Marcus MP, Marcinkiewicz MA, Santorini B (1 99 3) Building a large annotated corpus of english: the penn treebank. Comput Linguist 1 9(2):3 13 – 3 30",1,"7 3,0 00 tokens for validation, and 8 2,0 00 tokens for testing purposes. Its context is limited since it comprises sentences rather than paragraphs [ 7 6 ]. d) The Ministry of Electronics and Information Technology ’ s Technology Development Programme for Indian Languages (TDIL) launched its own data dis"
77,"Mc Callum A, Nigam K (1 99 8) A comparison of event models for naive bayes text classification. In AAAI- 9 8 workshop on learning for text categorization (Vol. 7 52, pp. 4 1-4 8)",2,"ts performance despite its simplicity (Lewis, 1 99 8) [ 6 7 ] In Text Categorization two types of models have been used (Mc Callum and Nigam, 1 99 8) [ 7 7 ]. Both modules assume that a fixed vocabulary is present. But in first model a document is generated by first choosing a subset of vocabulary and then"
77,"Mc Callum A, Nigam K (1 99 8) A comparison of event models for naive bayes text classification. In AAAI- 9 8 workshop on learning for text categorization (Vol. 7 52, pp. 4 1-4 8)",2,"ms you might have looked on an online store with discounts. In Information Retrieval two types of models have been used (Mc Callum and Nigam, 1 99 8) [ 7 7 ]. Both modules assume that a fixed vocabulary is present. But in first model a document is generated by first choosing a subset of vocabulary and then"
84,"Mc Gray AT, Sponsler JL, Brylawski B, Browne AC (1 98 7) The role of lexical knowledge in biomedical text understanding. In proceedings of the annual symposium on computer application in medical care (p. 1 03). Am Med Inform Assoc",1,"y medicine while highlighting or flagging data items [ 1 14 ]. The National Library of Medicine is developing The Specialist System [ 7 8 – 8 0 , 8 2 , 8 4 ]. It is expected to function as an Information Extraction tool for Biomedical Knowledge Bases, particularly Medline abstracts. 3 72 7 Multimedia Tools"
85,"Mc Keown KR (1 98 5) Text generation. Cambridge University Press, Cambridge",1,"r with phonological assumptions on which it is based and the evidence from which they have drawn their proposals. At the same time, Mc Keown (1 98 5) [ 8 5 ] demonstrated that rhetorical schemas could be used for producing both linguistically coherent and communica- tively effective text. Some research in"
86,"Merity S, Keskar NS, Socher R (2 01 8) An analysis of neural language modeling at multiple scales. ar Xiv preprint ar Xiv:1 80 3.0 82 40",1,oaches for dealing with relational reasoning on compartmentalized information. The results achieved with RMC show improved performance. Merity et al. [ 8 6 ] extended conventional word-level language models based on Quasi- Recurrent Neural Network and LSTM to handle the granularity at character and word le
87,"Mikolov T, Chen K, Corrado G., & Dean, J. (2 01 3). Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems",1,"eld of NLP, where two convolutional models with max pooling were used to perform parts-of-speech and named entity recognition tagging. Mikolov et.al. [ 8 7 ] proposed a word embedding process where the dense vector representation of text was addressed. They also report the challenges faced by traditional s"
88,"Morel-Guillemaz AM, Baud RH, Scherrer JR (1 99 0) Proximity processing of medical text. In medical informatics Europe ’ 9 0 (pp. 6 25 – 6 30). Springer, Berlin Heidelberg",1,", 9 4 , 1 13 ], and finally, a proper NLP system called RECIT [ 9 , 1 1 , 1 7 , 1 06 ] has been developed using a method called Proximity Processing [ 8 8 ]. It ’ s task was to implement a robust and multilingual system able to analyze/comprehend medical sentences, and to preserve a knowledge of free text"
89,Morin E (1 99 9) Automatic acquisition of semantic relations between terms from technical corpora. In proc. of the fifth international congress on terminology and knowledge engineering-TKE ’ 9 9,1,"terms is still a difficulty. PROMETHEE is a system that extracts lexico-syntactic patterns relative to a specific conceptual relation (Morin,1 99 9) [ 8 9 ]. IE systems should work at many levels, from word recognition to discourse analysis at the level of the complete document. An application of the Blan"
90,"Müller M, Salathé M, Kummervold PE (2 02 0) Covid-twitter-bert: A natural language processing model to analyse covid-1 9 content on twitter. ar Xiv preprint ar Xiv:2 00 5.0 75 03 9 1. ""Natural Language Processing (2 01 7) "" Natural Language Processing RSS. N.p., n.d. Web. 2 5 9 2. ""Natural Language Processing"" (2 01 7) Natural Language Processing RSS. N.p., n.d. Web. 2 3",1,have one vector representation for “ bank ” in both the sentences whereas BERT will have different vector representation for “ bank ” . Muller et al. [ 9 0 ] used the BERT model to analyze the tweets on covid-1 9 content. The use of the BERT model in the legal domain was explored by Chalkidis et al. [ 2 0
91,"""Natural Language Processing (2 01 7) "" Natural Language Processing RSS. N.p., n.d. Web. 2 5",1,"se forms the situation, while selecting subset of propositions that speaker has. The only requirement is the speaker must make sense of the situation [ 9 1 ]. ## 3 NLP: Then and now In the late 1 94 0 s the term NLP wasn ’ t in existence, but the work regarding machine translation (MT) had started. In fac"
92,"""Natural Language Processing"" (2 01 7) Natural Language Processing RSS. N.p., n.d. Web. 2 3",1,g mechanism linguistics knowledge is directly encoded in rule or other forms of representation. This helps the automatic process of natural languages [ 9 2 ]. Statistical and machine learning entail evolution of algorithms that allow a program to infer patterns. An iterative process is used to characterize
93,Newatia R (2 01 9) https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural- networks-ddad 72 c 70 48 c . Accessed 1 5 Dec 2 02 1,1,"Text Classification [ 1 18 ], Text Summarization [ 1 58 ], Machine Translation [ 7 0 ] and Answer Relations [ 1 50 ] . An article by Newatia (2 01 9) [ 9 3 ] illustrates the general architecture behind any CNN model, and how it can be used in the context of NLP. One can also refer to the work of Wang and G"
95,"Nießen S, Och FJ, Leusch G, Ney H (2 00 0) An evaluation tool for machine translation: fast evaluation for MT research. In LREC",1,"llmann et al., 1 99 7) [ 1 38 ], generation string accuracy (Bangalore et al., 2 00 0) [ 8 ], multi-reference word error rate (Nießen et al., 2 00 0) [ 9 5 ], BLEU score (Papineni et al., 2 00 2) [ 1 01 ], NIST score (Doddington, 2 00 2) [ 3 5 ] All these criteria try to approximate human assessment and of"
96,"Ochoa, A. (2 01 6). Meet the Pilot: Smart Earpiece Language Translator. https://www.indiegogo.com/ projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel . Accessed April 1 0, 2 01 7",1,"served settings. Link: https://www.ncbi.nlm.nih.gov/pubmed/2 82 69 89 5?dopt=Abstract e) Meet the Pilot, world ’ s first language translating earbuds [ 9 6 ] The world ’ s first smart earpiece Pilot will soon be transcribed over 1 5 languages. According to Spring wise, Waverly Labs ’ Pilot can already tran"
97,"Ogallo, W., & Kanter, A. S. (2 01 7). Using natural language processing and network analysis to develop a conceptual framework for medication therapy management research. https://www.ncbi.nlm.nih.gov/ pubmed/2 82 69 89 5?dopt=Abstract . Accessed April 1 0, 2 01 7",1,sing-future-bi d) Using Natural Language Processing and Network Analysis to Develop a Conceptual Framework for Medication Therapy Management Research [ 9 7 ] Natural Language Processing and Network Analysis to Develop a Conceptual Framework for Medication Therapy Management Research describes a theory deri
98,"Otter DW, Medina JR, Kalita JK (2 02 0) A survey of the usages of deep learning for natural language processing. IEEE Transactions on Neural Networks and Learning Systems 3 2(2):6 04 – 6 24",1,"del which compresses memories for long-range sequence learning, may be helpful for the readers. One may also refer to the recent work by Otter et al. [ 9 8 ] on uses of Deep Learning for NLP, and Fig. 3 A walkthrough of recent developments in NLP 3 72 2 Multimedia Tools and Applications (2 02 3) 8 2:3 71 3"
99,"Ouyang Y, Li W, Li S, Lu Q (2 01 1) Applying regression models to query-focused multi-document summarization. Inf Process Manag 4 7(2):2 27 – 2 37",1,also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani and Maybury 1 99 9 [ 7 4 ]; Fattah and Ren 2 00 9 [ 4 3 ]; Riedhammer et al. 2 01
