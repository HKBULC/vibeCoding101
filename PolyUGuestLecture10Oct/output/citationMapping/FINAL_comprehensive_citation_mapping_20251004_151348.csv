Reference_Number,Reference_Text,Citation_Count,Context_1,Context_2,Context_3,All_Match_Texts,Pattern_Types,Paragraph_Locations
1,"Ahonen H, Heinonen O, Klemettinen M, Verkamo AI (1 99 8) Applying data mining techniques for",4,e ’ s Intelligent Text Analyzer) (Glasgow et al. (1 99 8) [ 4 8 ]) that extracts information from life insurance applications. Ahonen et al. (1 99 8) [ 1 ] suggested a mainstream framework for text mining that uses pragmatic and discourse level analyses of text . e) Summarization Overload of informa...,e ’ s Intelligent Text Analyzer) (Glasgow et al. (1 99 8) [ 4 8 ]) that extracts information from life insurance applications. Ahonen et al. (1 99 8) [ 1 ] suggested a mainstream framework for text mining that uses pragmatic and discourse level analyses of text . e) Summarization Overload of informa...,A (Metlife ’ s Intelligent Text Analyzer) (Glasgow et al. (1 99 8) [ 4 8 ]) that extracts information from life insurance applications. Ahonen et al. (1 99 8) [ 1 ] suggested a mainstream framework for text mining that uses pragmatic and discourse level analyses of text . e) Summarization Overload o...,[ 1 ] | [ 1 ] | (1 99 8) [ 1 ] | Ahonen et al. (1 99 8) [ 1 ],Format_1_Pattern_1 | Format_1_Pattern_2 | Format_1_Pattern_5 | Format_1_Pattern_6,"40, 40, 40, 40"
2,Alshawi H (1 99 2) The core language engine. MIT press,4,"ike emphasis and themes. By the end of the decade the powerful general purpose sentence processors like SRI ’ s Core Language Engine (Alshawi,1 99 2) [ 2 ] and Discourse Representation Theory (Kamp and Reyle,1 99 3) [ 6 2 ] offered a means of tackling more extended discourse within the grammatico-lo...","ike emphasis and themes. By the end of the decade the powerful general purpose sentence processors like SRI ’ s Core Language Engine (Alshawi,1 99 2) [ 2 ] and Discourse Representation Theory (Kamp and Reyle,1 99 3) [ 6 2 ] offered a means of tackling more extended discourse within the grammatico-lo...","with functions like emphasis and themes. By the end of the decade the powerful general purpose sentence processors like SRI ’ s Core Language Engine (Alshawi,1 99 2) [ 2 ] and Discourse Representation Theory (Kamp and Reyle,1 99 3) [ 6 2 ] offered a means of tackling more extended discourse within t...","[ 2 ] | [ 2 ] | (Alshawi,1 99 2) [ 2 ] | LUNAR in interfacing the large databases. In
early 1 98 0 s computational grammar theory became a very active area of research linked with
logics for meaning and knowledge ’ s ability to deal with the user ’ s beliefs and intentions and
with functions like emphasis and themes.
By the end of the decade the powerful general purpose sentence processors like SRI ’ s Core
Language Engine (Alshawi,1 99 2) [ 2 ]",Format_2_Pattern_1 | Format_2_Pattern_2 | Format_2_Pattern_5 | Format_2_Pattern_6,"26, 26, 26, 26"
3,"Alshemali B, Kalita J (2 02 0) Improving the reliability of deep neural networks in NLP: A review. Knowl-",4,"led literature survey and technological challenges relevant to management research and NLP, and a recent review work by Alshemali and Kalita (2 02 0) [ 3 ], and references cited there in. In the recent past, models dealing with Visual Commonsense Reasoning [ 3 1 ] and NLP have also been getting atte...","led literature survey and technological challenges relevant to management research and NLP, and a recent review work by Alshemali and Kalita (2 02 0) [ 3 ], and references cited there in. In the recent past, models dealing with Visual Commonsense Reasoning [ 3 1 ] and NLP have also been getting atte...","for detailed literature survey and technological challenges relevant to management research and NLP, and a recent review work by Alshemali and Kalita (2 02 0) [ 3 ], and references cited there in. In the recent past, models dealing with Visual Commonsense Reasoning [ 3 1 ] and NLP have also been get...","[ 3 ] | [ 3 ] | (2 02 0) [ 3 ] | NLP, and a recent review work by Alshemali and Kalita (2 02 0) [ 3 ]",Format_3_Pattern_1 | Format_3_Pattern_2 | Format_3_Pattern_5 | Format_3_Pattern_6,"70, 70, 70, 70"
4,Andreev ND (1 96 7) The intermediary language as the focal point of machine translation. In: Booth AD,4,") had started. In fact, Research in this period was not completely localized. Russian and English were the dominant languages for MT (Andreev,1 96 7) [ 4 ]. In fact, MT/NLP research almost died in 1 96 6 according to the ALPAC report, which concluded that MT is going nowhere. But later, some MT prod...",") had started. In fact, Research in this period was not completely localized. Russian and English were the dominant languages for MT (Andreev,1 96 7) [ 4 ]. In fact, MT/NLP research almost died in 1 96 6 according to the ALPAC report, which concluded that MT is going nowhere. But later, some MT prod...","e translation (MT) had started. In fact, Research in this period was not completely localized. Russian and English were the dominant languages for MT (Andreev,1 96 7) [ 4 ]. In fact, MT/NLP research almost died in 1 96 6 according to the ALPAC report, which concluded that MT is going nowhere. But la...","[ 4 ] | [ 4 ] | (Andreev,1 96 7) [ 4 ] | In the late 1 94 0 s the term NLP wasn ’ t in existence, but the work regarding machine translation
(MT) had started. In fact, Research in this period was not completely localized. Russian and
English were the dominant languages for MT (Andreev,1 96 7) [ 4 ]",Format_4_Pattern_1 | Format_4_Pattern_2 | Format_4_Pattern_5 | Format_4_Pattern_6,"26, 26, 26, 26"
5,"Androutsopoulos I, Paliouras G, Karkaletsis V, Sakkis G, Spyropoulos CD, Stamatopoulos P (2 00 0)",8,"a document. Most text categorization approaches to anti-spam Email filtering have used multi variate Bernoulli model (Androutsopoulos et al., 2 00 0) [ 5 ] [ 1 5 ]. d) Information Extraction Information extraction is concerned with identifying phrases of interest of textual data. For many applicatio...","or Anti-Spam Filtering like Rule Learning (Cohen 1 99 6) [ 2 7 ], Naïve Bayes (Sahami et al., 1 99 8; Androutsopoulos et al., 2 00 0; Rennie.,2 00 0) [ 5 , 1 09 , 1 15 ],Memory based Learning (Sakkiset al.,2 00 0 b) [ 1 17 ], Support vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (C...","a document. Most text categorization approaches to anti-spam Email filtering have used multi variate Bernoulli model (Androutsopoulos et al., 2 00 0) [ 5 ] [ 1 5 ]. d) Information Extraction Information extraction is concerned with identifying phrases of interest of textual data. For many applicatio...","[ 5 ] | [ 5 ,
1 09 , 1 15 ] | [ 5 ] | (Androutsopoulos et al., 2 00 0) [ 5 ] | Both modules assume that a fixed vocabulary is present. But in first model a document is
generated by first choosing a subset of vocabulary and then using the selected words any
number of times, at least once irrespective of order. This is called Multi-variate Bernoulli
model. It takes the information of which words are used in a document irrespective of number
of words and order. In second model, a document is generated by choosing a set of word
occurrences and arranging them in any order. This model is called multi-nomial model, in
addition to the Multi-variate Bernoulli model, it also captures information on how many times
a word is used in a document. Most text categorization approaches to anti-spam Email filtering
have used multi variate Bernoulli model (Androutsopoulos et al., 2 00 0) [ 5 ] | [5] | [5] | In Degoulet P, Stephan JC, Venot A, Yvon PJ, rédacteurs. Informatique
et Santé, Informatique et Gestion des Unités de Soins, Comptes Rendus du Colloque AIM-IF, Paris (pp.
2 46 – 5 6). [5]",Format_5_Pattern_1 | Format_5_Pattern_2 | Format_5_Pattern_2 | Format_5_Pattern_5 | Format_5_Pattern_6 | Format_5_Pattern_1 | Format_5_Pattern_2 | Format_5_Pattern_6,"38, 38, 38, 38, 38, 93, 93, 93"
6,"Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J (2 02 0) Artificial intelligence in public",4,"e work of Sharifirad and Matwin (2 01 9) [ 1 23 ] for classification of different online harassment categories and challenges, Baclic et.al. (2 02 0) [ 6 ] and Wong et al. (2 01 8) [ 1 51 ] for challenges and opportunities in public health, Kang et.al. (2 02 0) [ 6 3 ] for detailed literature survey...","e work of Sharifirad and Matwin (2 01 9) [ 1 23 ] for classification of different online harassment categories and challenges, Baclic et.al. (2 02 0) [ 6 ] and Wong et al. (2 01 8) [ 1 51 ] for challenges and opportunities in public health, Kang et.al. (2 02 0) [ 6 3 ] for detailed literature survey...","fer to the work of Sharifirad and Matwin (2 01 9) [ 1 23 ] for classification of different online harassment categories and challenges, Baclic et.al. (2 02 0) [ 6 ] and Wong et al. (2 01 8) [ 1 51 ] for challenges and opportunities in public health, Kang et.al. (2 02 0) [ 6 3 ] for detailed literatu...",[ 6 ] | [ 6 ] | (2 02 0) [ 6 ] | Baclic et.al. (2 02 0) [ 6 ],Format_6_Pattern_1 | Format_6_Pattern_2 | Format_6_Pattern_5 | Format_6_Pattern_6,"70, 70, 70, 70"
7,"Bahdanau D, Cho K, Bengio Y (2 01 5) Neural machine translation by jointly learning to align and",3,"variant, called the gated recurrent unit (GRU), which has shown better results than standard LSTMs in many tasks [ 2 2 , 2 6 ]. Attention mechanisms [ 7 ] which suggest a network to learn what to pay attention to in accordance with the current hidden state and annotation together with the use of tra...","variant, called the gated recurrent unit (GRU), which has shown better results than standard LSTMs in many tasks [ 2 2 , 2 6 ]. Attention mechanisms [ 7 ] which suggest a network to learn what to pay attention to in accordance with the current hidden state and annotation together with the use of tra...","to a slightly simpler variant, called the gated recurrent unit (GRU), which has shown better results than standard LSTMs in many tasks [ 2 2 , 2 6 ]. Attention mechanisms [ 7 ] which suggest a network to learn what to pay attention to in accordance with the current hidden state and annotation togeth...",[ 7 ] | [ 7 ] | Attention mechanisms [ 7 ],Format_7_Pattern_1 | Format_7_Pattern_2 | Format_7_Pattern_6,"31, 31, 31"
8,"Bangalore S, Rambow O, Whittaker S (2 00 0) Evaluation metrics for generation. In proceedings of the first",4,"s are word error rate, position-independent word error rate (Tillmann et al., 1 99 7) [ 1 38 ], generation string accuracy (Bangalore et al., 2 00 0) [ 8 ], multi-reference word error rate (Nießen et al., 2 00 0) [ 9 5 ], BLEU score (Papineni et al., 2 00 2) [ 1 01 ], NIST score (Doddington, 2 00 2)...","s are word error rate, position-independent word error rate (Tillmann et al., 1 99 7) [ 1 38 ], generation string accuracy (Bangalore et al., 2 00 0) [ 8 ], multi-reference word error rate (Nießen et al., 2 00 0) [ 9 5 ], BLEU score (Papineni et al., 2 00 2) [ 1 01 ], NIST score (Doddington, 2 00 2)...","ns. Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1 99 7) [ 1 38 ], generation string accuracy (Bangalore et al., 2 00 0) [ 8 ], multi-reference word error rate (Nießen et al., 2 00 0) [ 9 5 ], BLEU score (Papineni et al., 2 00 2) [ 1 01 ], NIST...","[ 8 ] | [ 8 ] | (Bangalore et al., 2 00 0) [ 8 ] | Bangalore et al., 2 00 0) [ 8 ]",Format_8_Pattern_1 | Format_8_Pattern_2 | Format_8_Pattern_5 | Format_8_Pattern_6,"36, 36, 36, 36"
9,"Baud RH, Rassinoux AM, Scherrer JR (1 99 1) Knowledge representation of discharge summaries. In",1,"cords were archived. At later stage the LSP-MLP has been adapted for French [ 1 0 , 7 2 , 9 4 , 1 13 ], and finally, a proper NLP system called RECIT [ 9 , 1 1 , 1 7 , 1 06 ] has been developed using a method called Proximity Processing [ 8 8 ]. It ’ s task was to implement a robust and multilingual...",,,"[ 9 , 1 1 , 1 7 , 1 06 ]",Format_9_Pattern_2,44
10,"Baud RH, Rassinoux AM, Scherrer JR (1 99 2) Natural language processing and semantical representation",1,"vironment with NLP features [ 8 1 , 1 19 ]. In the first phase, patient records were archived. At later stage the LSP-MLP has been adapted for French [ 1 0 , 7 2 , 9 4 , 1 13 ], and finally, a proper NLP system called RECIT [ 9 , 1 1 , 1 7 , 1 06 ] has been developed using a method called Proximity ...",,,"[ 1 0 , 7 2 , 9 4 , 1 13 ]",Format_1 0_Pattern_2,44
11,"Baud RH, Alpay L, Lovis C (1 99 4) Let ’ s meet the users with natural language understanding. Knowledge",1,"cords were archived. At later stage the LSP-MLP has been adapted for French [ 1 0 , 7 2 , 9 4 , 1 13 ], and finally, a proper NLP system called RECIT [ 9 , 1 1 , 1 7 , 1 06 ] has been developed using a method called Proximity Processing [ 8 8 ]. It ’ s task was to implement a robust and multilingual...",,,"[ 9 , 1 1 , 1 7 , 1 06 ]",Format_1 1_Pattern_2,44
12,"Bengio Y, Ducharme R, Vincent P (2 00 1) A neural probabilistic language model. Proceedings of NIPS",3,"arly 2 00 0, neural language modeling in which the probability of occurring of next word (token) is determined given n previous words. Bendigo et al. [ 1 2 ] proposed the concept of feed forward neural network and lookup table which represents the n previous words in sequence. Collobert et al. [ 2 9...","arly 2 00 0, neural language modeling in which the probability of occurring of next word (token) is determined given n previous words. Bendigo et al. [ 1 2 ] proposed the concept of feed forward neural network and lookup table which represents the n previous words in sequence. Collobert et al. [ 2 9...","The main objectives of NLP include interpretation, analysis, and manipulation of natural language data for the intended purpose with the use of various algorithms, tools, and methods. However, there are many challenges involved which may depend upon the natural language data under consideration, and...","[ 1 2 ] | [ 1 2 ] | The main objectives of NLP include interpretation, analysis, and manipulation of natural
language data for the intended purpose with the use of various algorithms, tools, and methods.
However, there are many challenges involved which may depend upon the natural language
data under consideration, and so makes it difficult to achieve all the objectives with a single
approach. Therefore, the development of different tools and methods in the field of NLP and
relevant areas of studies have received much attention from several researchers in the recent
past. The developments can be seen in the Fig. 3 :
In early 2 00 0, neural language modeling in which the probability of occurring of next word
(token) is determined given n previous words. Bendigo et al. [ 1 2 ]",Format_1 2_Pattern_1 | Format_1 2_Pattern_2 | Format_1 2_Pattern_6,"29, 29, 29"
13,"Benson E, Haghighi A, Barzilay R (2 01 1) Event discovery in social media feeds. In proceedings of the",4,"a given verb, and finally classifying these nodes to compute the corresponding SRL tags. Event discovery in social media feeds (Benson et al.,2 01 1) [ 1 3 ], using a graphical model to analyze any social media feeds to determine whether it contains the name of a person or name of a venue, place, ti...","a given verb, and finally classifying these nodes to compute the corresponding SRL tags. Event discovery in social media feeds (Benson et al.,2 01 1) [ 1 3 ], using a graphical model to analyze any social media feeds to determine whether it contains the name of a person or name of a venue, place, ti...","esent the arguments of a given verb, and finally classifying these nodes to compute the corresponding SRL tags. Event discovery in social media feeds (Benson et al.,2 01 1) [ 1 3 ], using a graphical model to analyze any social media feeds to determine whether it contains the name of a person or nam...","[ 1 3 ] | [ 1 3 ] | (Benson et al.,2 01 1) [ 1 3 ] | The precise arguments depend on the verb frame and if
multiple verbs exist in a sentence, it might have multiple tags. State-of-the-art SRL systems
comprise several stages: creating a parse tree, identifying which parse tree nodes represent the
arguments of a given verb, and finally classifying these nodes to compute the corresponding
SRL tags.
Event discovery in social media feeds (Benson et al.,2 01 1) [ 1 3 ]",Format_1 3_Pattern_1 | Format_1 3_Pattern_2 | Format_1 3_Pattern_5 | Format_1 3_Pattern_6,"35, 35, 35, 35"
14,"Berger AL, Della Pietra SA, Della Pietra VJ (1 99 6) A maximum entropy approach to natural language",4,"vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding method (T. Xia, 2 02 0) [ 1 53 ], sometimes combining different learners (Sakkis et al., 2 00 1) [ 1 16 ]. Usi...","vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding method (T. Xia, 2 02 0) [ 1 53 ], sometimes combining different learners (Sakkis et al., 2 00 1) [ 1 16 ]. Usi...","0 b) [ 1 17 ], Support vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding method (T. Xia, 2 02 0) [ 1 53 ], sometimes combining different learners (Sakkis et al....","[ 1 4 ] | [ 1 4 ] | (Berger et al. 1 99 6) [ 1 4 ] | Maximum
Entropy Model (Berger et al. 1 99 6) [ 1 4 ]",Format_1 4_Pattern_1 | Format_1 4_Pattern_2 | Format_1 4_Pattern_5 | Format_1 4_Pattern_6,"38, 38, 38, 38"
15,"Blanzieri E, Bryl A (2 00 8) A survey of learning-based techniques of email spam filtering. Artif Intell Rev",3,"ment. Most text categorization approaches to anti-spam Email filtering have used multi variate Bernoulli model (Androutsopoulos et al., 2 00 0) [ 5 ] [ 1 5 ]. d) Information Extraction Information extraction is concerned with identifying phrases of interest of textual data. For many applications, ex...","ment. Most text categorization approaches to anti-spam Email filtering have used multi variate Bernoulli model (Androutsopoulos et al., 2 00 0) [ 5 ] [ 1 5 ]. d) Information Extraction Information extraction is concerned with identifying phrases of interest of textual data. For many applications, ex...","that even though a great amount of work on natural language processing is available in literature surveys (one may refer to [ 1 5 , 3 2 , 6 3 , 9 8 , 1 33 , 1 51 ] focusing on one domain such as usage of deep-learning techniques in NLP, techniques used for email spam filtering, medication safety, ma...","[ 1 5 ] | [ 1 5 ] | [ 1 5 , 3 2 , 6 3 , 9 8 , 1 33 , 1 51 ]",Format_1 5_Pattern_1 | Format_1 5_Pattern_2 | Format_1 5_Pattern_2,"38, 38, 73"
16,"Bondale N, Maloor P, Vaidyanathan A, Sengupta S, Rao PV (1 99 9) Extraction of information from open-",4,"on to discourse analysis at the level of the complete document. An application of the Blank Slate Language Processor (BSLP) ( Bondale et al., 1 99 9) [ 1 6 ] approach for the analysis of a real-life natural language corpus that consists of responses to open-ended questionnaires in the field of adver...","on to discourse analysis at the level of the complete document. An application of the Blank Slate Language Processor (BSLP) ( Bondale et al., 1 99 9) [ 1 6 ] approach for the analysis of a real-life natural language corpus that consists of responses to open-ended questionnaires in the field of adver...","evels, from word recognition to discourse analysis at the level of the complete document. An application of the Blank Slate Language Processor (BSLP) ( Bondale et al., 1 99 9) [ 1 6 ] approach for the analysis of a real-life natural language corpus that consists of responses to open-ended questionna...","[ 1 6 ] | [ 1 6 ] | ( Bondale et al., 1 99 9) [ 1 6 ] | IE systems should work at many levels, from
word recognition to discourse analysis at the level of the complete document. An application
of the Blank Slate Language Processor (BSLP) ( Bondale et al., 1 99 9) [ 1 6 ]",Format_1 6_Pattern_1 | Format_1 6_Pattern_2 | Format_1 6_Pattern_5 | Format_1 6_Pattern_6,"40, 40, 40, 40"
17,"Borst F, Sager N, Nhàn NT, Su Y, Lyman M, Tick LJ, ..., Scherrer JR (1 98 9) Analyse automatique de",1,"cords were archived. At later stage the LSP-MLP has been adapted for French [ 1 0 , 7 2 , 9 4 , 1 13 ], and finally, a proper NLP system called RECIT [ 9 , 1 1 , 1 7 , 1 06 ] has been developed using a method called Proximity Processing [ 8 8 ]. It ’ s task was to implement a robust and multilingual...",,,"[ 9 , 1 1 , 1 7 , 1 06 ]",Format_1 7_Pattern_2,44
18,"Briscoe EJ, Grover C, Boguraev B, Carroll J (1 98 7) A formalism and environment for the development of",4,"mmunities. Practical resources, grammars, and tools and parsers became available (for example: Alvey Natural Language Tools) (Briscoe et al., 1 98 7) [ 1 8 ]. The (D)ARPA speech recognition and message understanding (information extraction) conferences were not only for the tasks they addressed 3 72...","mmunities. Practical resources, grammars, and tools and parsers became available (for example: Alvey Natural Language Tools) (Briscoe et al., 1 98 7) [ 1 8 ]. The (D)ARPA speech recognition and message understanding (information extraction) conferences were not only for the tasks they addressed 3 72...","was one of the growing communities. Practical resources, grammars, and tools and parsers became available (for example: Alvey Natural Language Tools) (Briscoe et al., 1 98 7) [ 1 8 ]. The (D)ARPA speech recognition and message understanding (information extraction) conferences were not only for the ...","[ 1 8 ] | [ 1 8 ] | (Briscoe et al., 1 98 7) [ 1 8 ] | This period was one of the growing communities. Practical
resources, grammars, and tools and parsers became available (for example: Alvey Natural
Language Tools) (Briscoe et al., 1 98 7) [ 1 8 ]",Format_1 8_Pattern_1 | Format_1 8_Pattern_2 | Format_1 8_Pattern_5 | Format_1 8_Pattern_6,"26, 26, 26, 26"
19,"Carreras X, Marquez L (2 00 1) Boosting trees for anti-spam email filtering. ar Xiv preprint cs/0 10 90 15",4,"sed Learning (Sakkiset al.,2 00 0 b) [ 1 17 ], Support vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding method (T. Xia, 2 02 0) [ 1 53 ], sometimes combining d...","sed Learning (Sakkiset al.,2 00 0 b) [ 1 17 ], Support vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding method (T. Xia, 2 02 0) [ 1 53 ], sometimes combining d...",") [ 5 , 1 09 , 1 15 ],Memory based Learning (Sakkiset al.,2 00 0 b) [ 1 17 ], Support vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding method (T. Xia, 2 02 0) ...","[ 1 9 ] | [ 1 9 ] | (Carreras and Marquez, 2 00 1) [ 1 9 ] | Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ]",Format_1 9_Pattern_1 | Format_1 9_Pattern_2 | Format_1 9_Pattern_5 | Format_1 9_Pattern_6,"38, 38, 38, 38"
20,"Chalkidis I, Fergadiotis M, Malakasiotis P, Aletras N, Androutsopoulos I (2 02 0) LEGAL-BERT: the",3,"9 0 ] used the BERT model to analyze the tweets on covid-1 9 content. The use of the BERT model in the legal domain was explored by Chalkidis et al. [ 2 0 ]. Since BERT considers up to 5 12 tokens, this is the reason if there is a long text sequence that must be divided into multiple short text sequ...","9 0 ] used the BERT model to analyze the tweets on covid-1 9 content. The use of the BERT model in the legal domain was explored by Chalkidis et al. [ 2 0 ]. Since BERT considers up to 5 12 tokens, this is the reason if there is a long text sequence that must be divided into multiple short text sequ...",epresentation for “ bank ” in both the sentences whereas BERT will have different vector representation for “ bank ” . Muller et al. [ 9 0 ] used the BERT model to analyze the tweets on covid-1 9 content. The use of the BERT model in the legal domain was explored by Chalkidis et al. [ 2 0 ]. Since B...,"[ 2 0 ] | [ 2 0 ] | BERT model to analyze the tweets on covid-1 9 content.
The use of the BERT model in the legal domain was explored by Chalkidis et al. [ 2 0 ]",Format_2 0_Pattern_1 | Format_2 0_Pattern_2 | Format_2 0_Pattern_6,"63, 63, 63"
21,"Chi EC, Lyman MS, Sager N, Friedman C, Macleod C (1 98 5) A database of computer-structured",1,"pplied in the field as well. The Linguistic String Project-Medical Language Processor is one the large scale projects of NLP in the field of medicine [ 2 1 , 5 3 , 5 7 , 7 1 , 1 14 ]. The LSP- MLP helps enabling physicians to extract and summarize information of any signs or symptoms, drug dosage an...",,,"[ 2 1 , 5 3 , 5 7 , 7 1 , 1 14 ]",Format_2 1_Pattern_2,42
22,"Cho K, Van Merriënboer B, Bahdanau D, Bengio Y, (2 01 4) On the properties of neural machine",1,"M has also led to a slightly simpler variant, called the gated recurrent unit (GRU), which has shown better results than standard LSTMs in many tasks [ 2 2 , 2 6 ]. Attention mechanisms [ 7 ] which suggest a network to learn what to pay attention to in accordance with the current hidden state and an...",,,"[ 2 2 , 2 6 ]",Format_2 2_Pattern_2,31
23,"Chomsky N (1 96 5) Aspects of the theory of syntax. MIT Press, Cambridge, Massachusetts",4,"d syntactic theories, marked a unique position in the field of theoretical linguistics because he revolutionized the area of syntax (Chomsky, 1 96 5) [ 2 3 ]. Further, Natural Language Genera- tion (NLG) is the process of producing phrases, sentences and paragraphs that are meaningful from an intern...","d syntactic theories, marked a unique position in the field of theoretical linguistics because he revolutionized the area of syntax (Chomsky, 1 96 5) [ 2 3 ]. Further, Natural Language Genera- tion (NLG) is the process of producing phrases, sentences and paragraphs that are meaningful from an intern...","entury that started syntactic theories, marked a unique position in the field of theoretical linguistics because he revolutionized the area of syntax (Chomsky, 1 96 5) [ 2 3 ]. Further, Natural Language Genera- tion (NLG) is the process of producing phrases, sentences and paragraphs that are meaning...","[ 2 3 ] | [ 2 3 ] | (Chomsky, 1 96 5) [ 2 3 ] | Since all the users may not be
well-versed in machine specific language, N atural Language Processing (NLP) caters those
users who do not have enough time to learn new languages or get perfection in it. In fact, NLP
is a tract of Artificial Intelligence and Linguistics, devoted to make computers understand the
statements or words written in human languages. It came into existence to ease the user ’ s work
and to satisfy the wish to communicate with the computer in natural language, and can be
classified into two parts i.e. Natural Language Understanding or Linguistics and Natural
Language Generation which evolves the task to understand and generate the text. L inguistics
is the science of language which includes Phonology that refers to sound, Morphology word
formation, Syntax sentence structure, Semantics syntax and Pragmatics which refers to
understanding. Noah Chomsky, one of the first linguists of twelfth century that started
syntactic theories, marked a unique position in the field of theoretical linguistics because he
revolutionized the area of syntax (Chomsky, 1 96 5) [ 2 3 ]",Format_2 3_Pattern_1 | Format_2 3_Pattern_2 | Format_2 3_Pattern_5 | Format_2 3_Pattern_6,"10, 10, 10, 10"
24,Choudhary N (2 02 1) LDC-IL: the Indian repository of resources for language technology. Lang Resources,3,chnology Development Programme for Indian Languages (TDIL) launched its own data distribution portal ( www. tdil-dc.in ) which has cataloged datasets [ 2 4 ]. 3. Machine Translation: The task of converting the text of one natural language into another language while keeping the sense of the input te...,chnology Development Programme for Indian Languages (TDIL) launched its own data distribution portal ( www. tdil-dc.in ) which has cataloged datasets [ 2 4 ]. 3. Machine Translation: The task of converting the text of one natural language into another language while keeping the sense of the input te...,"kens for validation, and 8 2,0 00 tokens for testing purposes. Its context is limited since it comprises sentences rather than paragraphs [ 7 6 ]. d) The Ministry of Electronics and Information Technology ’ s Technology Development Programme for Indian Languages (TDIL) launched its own data distribu...","[ 2 4 ] | [ 2 4 ] | The Ministry of Electronics and Information Technology ’ s Technology Development
Programme for Indian Languages (TDIL) launched its own data distribution portal ( www.
tdil-dc.in ) which has cataloged datasets [ 2 4 ]",Format_2 4_Pattern_1 | Format_2 4_Pattern_2 | Format_2 4_Pattern_6,"53, 53, 53"
25,"Chouikhi H, Chniter H, Jarray F (2 02 1) Arabic sentiment analysis using BERT model. In international",1,"or various NLP tasks such as question answering, sentiment analysis, text classification, sentence embedding, interpreting ambiguity in the text etc. [ 2 5 , 3 3 , 9 0 , 1 48 ]. Earlier language-based models examine the text in either of one direction which is used for sentence generation by predict...",,,"[ 2 5 , 3 3 , 9 0 , 1 48 ]",Format_2 5_Pattern_2,63
26,"Chung J, Gulcehre C, Cho K, Bengio Y, (2 01 4) Empirical evaluation of gated recurrent neural networks",1,"M has also led to a slightly simpler variant, called the gated recurrent unit (GRU), which has shown better results than standard LSTMs in many tasks [ 2 2 , 2 6 ]. Attention mechanisms [ 7 ] which suggest a network to learn what to pay attention to in accordance with the current hidden state and an...",,,"[ 2 2 , 2 6 ]",Format_2 6_Pattern_2,31
27,Cohen WW (1 99 6) Learning rules that classify e-mail. In AAAI spring symposium on machine learning in,4,"n recent times, various machine learning techniques have been applied to text categorization or Anti-Spam Filtering like Rule Learning (Cohen 1 99 6) [ 2 7 ], Naïve Bayes (Sahami et al., 1 99 8; Androutsopoulos et al., 2 00 0; Rennie.,2 00 0) [ 5 , 1 09 , 1 15 ],Memory based Learning (Sakkiset al.,2...","n recent times, various machine learning techniques have been applied to text categorization or Anti-Spam Filtering like Rule Learning (Cohen 1 99 6) [ 2 7 ], Naïve Bayes (Sahami et al., 1 99 8; Androutsopoulos et al., 2 00 0; Rennie.,2 00 0) [ 5 , 1 09 , 1 15 ],Memory based Learning (Sakkiset al.,2...","orization and in recent times, various machine learning techniques have been applied to text categorization or Anti-Spam Filtering like Rule Learning (Cohen 1 99 6) [ 2 7 ], Naïve Bayes (Sahami et al., 1 99 8; Androutsopoulos et al., 2 00 0; Rennie.,2 00 0) [ 5 , 1 09 , 1 15 ],Memory based Learning ...","[ 2 7 ] | [ 2 7 ] | (Cohen
1 99 6) [ 2 7 ] | Reuters articles
and saves much time by doing the work that is to be done by staff or human indexers. Some
companies have been using categorization systems to categorize trouble tickets or complaint
requests and routing to the appropriate desks. Another application of text categorization is
email spam filters. Spam filters are becoming important as the first line of defence against the
unwanted emails. A false negative and false positive issue of spam filters is at the heart of NLP
technology, it has brought down the challenge of extracting meaning from strings of text. A
filtering solution that is applied to an email system uses a set of protocols to determine which
of the incoming messages are spam; and which are not. There are several types of spam filters
available. Content filters : Review the content within the message to determine whether it is
spam or not. Header filters : Review the email header looking for fake information. General
Blacklist filters : Stop all emails from blacklisted recipients. Rules Based Filters : It uses user-
defined criteria. Such as stopping mails from a specific person or stopping mail including a
specific word. Permission Filters : Require anyone sending a message to be pre-approved by
the recipient. Challenge Response Filters : Requires anyone sending a message to enter a code
to gain permission to send email.
c)
Spam Filtering
It works using text categorization and in recent times, various machine learning techniques
have been applied to text categorization or Anti-Spam Filtering like Rule Learning (Cohen
1 99 6) [ 2 7 ]",Format_2 7_Pattern_1 | Format_2 7_Pattern_2 | Format_2 7_Pattern_5 | Format_2 7_Pattern_6,"38, 38, 38, 38"
28,"Cohen PR, Morgan J, Ramsay AM (2 00 2) Intention in communication, Am J Psychol 1 04(4)",4,"inchor,1 99 3) [ 1 31 , 1 57 ]. Work on user modeling (Wahlster and Kobsa, 1 98 9) [ 1 42 ] was one strand in a research paper. Cohen et al. (2 00 2) [ 2 8 ] had put forwarded a first approximation of a compositional theory of tune interpretation, together with phonological assumptions on which it i...","inchor,1 99 3) [ 1 31 , 1 57 ]. Work on user modeling (Wahlster and Kobsa, 1 98 9) [ 1 42 ] was one strand in a research paper. Cohen et al. (2 00 2) [ 2 8 ] had put forwarded a first approximation of a compositional theory of tune interpretation, together with phonological assumptions on which it i...","im and Chinchor,1 99 3) [ 1 31 , 1 57 ]. Work on user modeling (Wahlster and Kobsa, 1 98 9) [ 1 42 ] was one strand in a research paper. Cohen et al. (2 00 2) [ 2 8 ] had put forwarded a first approximation of a compositional theory of tune interpretation, together with phonological assumptions on w...",[ 2 8 ] | [ 2 8 ] | (2 00 2) [ 2 8 ] | Cohen et al. (2 00 2) [ 2 8 ],Format_2 8_Pattern_1 | Format_2 8_Pattern_2 | Format_2 8_Pattern_5 | Format_2 8_Pattern_6,"28, 28, 28, 28"
29,"Collobert R, Weston J (2 00 8) A unified architecture for natural language processing. In proceedings of the",3,"al. [ 1 2 ] proposed the concept of feed forward neural network and lookup table which represents the n previous words in sequence. Collobert et al. [ 2 9 ] proposed the application of multitask learning in the field of NLP, where two convolutional models with max pooling were used to perform parts-...","al. [ 1 2 ] proposed the concept of feed forward neural network and lookup table which represents the n previous words in sequence. Collobert et al. [ 2 9 ] proposed the application of multitask learning in the field of NLP, where two convolutional models with max pooling were used to perform parts-...","words. Bendigo et al. [ 1 2 ] proposed the concept of feed forward neural network and lookup table which represents the n previous words in sequence. Collobert et al. [ 2 9 ] proposed the application of multitask learning in the field of NLP, where two convolutional models with max pooling were used...",[ 2 9 ] | [ 2 9 ] | Collobert et al. [ 2 9 ],Format_2 9_Pattern_1 | Format_2 9_Pattern_2 | Format_2 9_Pattern_6,"29, 29, 29"
30,"Dai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov R, (2 01 9) Transformer-xl: attentive language",3,learning longer-term dependency but are limited by a fixed-length context in the setting of language modeling. In this direction recently Dai et al. [ 3 0 ] proposed a novel neural architecture Transformer-XL (XL as extra- long) which enables learning dependencies beyond a fixed length of words. Fur...,learning longer-term dependency but are limited by a fixed-length context in the setting of language modeling. In this direction recently Dai et al. [ 3 0 ] proposed a novel neural architecture Transformer-XL (XL as extra- long) which enables learning dependencies beyond a fixed length of words. Fur...,"nce with the current hidden state and annotation together with the use of transformers have also made a significant development in NLP, see [ 1 41 ]. It is to be noticed that Transformers have a potential of learning longer-term dependency but are limited by a fixed-length context in the setting of ...","[ 3 0 ] | [ 3 0 ] | It is to be noticed that Transformers have a potential of learning longer-term dependency
but are limited by a fixed-length context in the setting of language modeling. In this direction
recently Dai et al. [ 3 0 ]",Format_3 0_Pattern_1 | Format_3 0_Pattern_2 | Format_3 0_Pattern_6,"31, 31, 31"
31,"Davis E, Marcus G (2 01 5) Commonsense reasoning and commonsense knowledge in artificial intelli-",3,"view work by Alshemali and Kalita (2 02 0) [ 3 ], and references cited there in. In the recent past, models dealing with Visual Commonsense Reasoning [ 3 1 ] and NLP have also been getting attention of the several researchers and seems a promising and challenging area to work upon. These models try ...","view work by Alshemali and Kalita (2 02 0) [ 3 ], and references cited there in. In the recent past, models dealing with Visual Commonsense Reasoning [ 3 1 ] and NLP have also been getting attention of the several researchers and seems a promising and challenging area to work upon. These models try ...","l challenges relevant to management research and NLP, and a recent review work by Alshemali and Kalita (2 02 0) [ 3 ], and references cited there in. In the recent past, models dealing with Visual Commonsense Reasoning [ 3 1 ] and NLP have also been getting attention of the several researchers and s...","[ 3 1 ] | [ 3 1 ] | In the recent past, models dealing with Visual Commonsense Reasoning [ 3 1 ]",Format_3 1_Pattern_1 | Format_3 1_Pattern_2 | Format_3 1_Pattern_6,"70, 70, 70"
32,"Desai NP, Dabhi VK (2 02 2) Resources and components for Gujarati NLP systems: a survey. Artif Intell",1,"that even though a great amount of work on natural language processing is available in literature surveys (one may refer to [ 1 5 , 3 2 , 6 3 , 9 8 , 1 33 , 1 51 ] focusing on one domain such as usage of deep-learning techniques in NLP, techniques used for email spam filtering, medication safety, ma...",,,"[ 1 5 , 3 2 , 6 3 , 9 8 , 1 33 , 1 51 ]",Format_3 2_Pattern_2,73
33,"Devlin J, Chang MW, Lee K, Toutanova K, (2 01 8) Bert: pre-training of deep bidirectional transformers for",5,"relevant references cited therein. The use of BERT (Bidirectional Encoder Representations from Transformers) [ 3 3 ] model and successive models have also played an important role for NLP. Many researchers worked on NLP, building tools and systems which makes NLP wh","relevant references cited therein. The use of BERT (Bidirectional Encoder Representations from Transformers) [ 3 3 ] model and successive models have also played an important role for NLP. Many researchers worked on NLP, building tools and systems which makes NLP wh","relevant references cited therein. The use of BERT (Bidirectional Encoder Representations from Transformers) [ 3 3 ] model and successive models have also played an important role for NLP. Many researchers worked on NLP, building tools and systems which makes NLP wh","[ 3 3 ] | [ 3 3 ] | (Bidirectional Encoder Representations
from Transformers) [ 3 3 ] | The use of BERT (Bidirectional Encoder Representations
from Transformers) [ 3 3 ] | [ 2 5 , 3 3 , 9 0 , 1 48 ]",Format_3 3_Pattern_1 | Format_3 3_Pattern_2 | Format_3 3_Pattern_5 | Format_3 3_Pattern_6 | Format_3 3_Pattern_2,"33, 33, 33, 33, 63"
34,"Diab M, Hacioglu K, Jurafsky D (2 00 4) Automatic tagging of Arabic text: From raw text to base phrase",4,"each word using suffix stripping algorithm, wherein the longest suffix is searched from the suffix table and tags are assigned. Diab et al. (2 00 4) [ 3 4 ] used supervised machine learning approach and adopted Support Vector Machines (SVMs) which were trained on the Arabic Treebank to automati- cal...","each word using suffix stripping algorithm, wherein the longest suffix is searched from the suffix table and tags are assigned. Diab et al. (2 00 4) [ 3 4 ] used supervised machine learning approach and adopted Support Vector Machines (SVMs) which were trained on the Arabic Treebank to automati- cal...","te tag to each word using suffix stripping algorithm, wherein the longest suffix is searched from the suffix table and tags are assigned. Diab et al. (2 00 4) [ 3 4 ] used supervised machine learning approach and adopted Support Vector Machines (SVMs) which were trained on the Arabic Treebank to aut...","[ 3 4 ] | [ 3 4 ] | (2 00 4) [ 3 4 ] | POS Tagger for Sanskrit
Language. Sanskrit sentences are parsed to assign the appropriate tag to each word using
suffix stripping algorithm, wherein the longest suffix is searched from the suffix table and tags
are assigned. Diab et al. (2 00 4) [ 3 4 ]",Format_3 4_Pattern_1 | Format_3 4_Pattern_2 | Format_3 4_Pattern_5 | Format_3 4_Pattern_6,"33, 33, 33, 33"
35,Doddington G (2 00 2) Automatic evaluation of machine translation quality using n-gram co-occurrence,5,"8 ], multi-reference word error rate (Nießen et al., 2 00 0) [ 9 5 ], BLEU score (Papineni et al., 2 00 2) [ 1 01 ], NIST score (Doddington, 2 00 2) [ 3 5 ] All these criteria try to approximate human assessment and often achieve an astonishing degree of correlation to human subjective evaluation of...","8 ], multi-reference word error rate (Nießen et al., 2 00 0) [ 9 5 ], BLEU score (Papineni et al., 2 00 2) [ 1 01 ], NIST score (Doddington, 2 00 2) [ 3 5 ] All these criteria try to approximate human assessment and often achieve an astonishing degree of correlation to human subjective evaluation of...","ten achieve an astonishing degree of correlation to human subjective evaluation of fluency and adequacy (Papineni et al., 2 00 1; Doddington, 2 00 2) [ 3 5 , 1 01 ]. 3 72 4 Multimedia Tools and Applications (2 02 3) 8 2:3 71 3–3 74 4","[ 3 5 ] | [ 3 5 ] | [ 3 5 , 1 01 ] | (Doddington, 2 00 2) [ 3 5 ] | NIST
score (Doddington, 2 00 2) [ 3 5 ]",Format_3 5_Pattern_1 | Format_3 5_Pattern_2 | Format_3 5_Pattern_2 | Format_3 5_Pattern_5 | Format_3 5_Pattern_6,"36, 36, 36, 36, 36"
36,"Drucker H, Wu D, Vapnik VN (1 99 9) Support vector machines for spam categorization. IEEE Trans",4,", 2 00 0; Rennie.,2 00 0) [ 5 , 1 09 , 1 15 ],Memory based Learning (Sakkiset al.,2 00 0 b) [ 1 17 ], Support vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding ...",", 2 00 0; Rennie.,2 00 0) [ 5 , 1 09 , 1 15 ],Memory based Learning (Sakkiset al.,2 00 0 b) [ 1 17 ], Support vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash Forest and a rule encoding ...","; Androutsopoulos et al., 2 00 0; Rennie.,2 00 0) [ 5 , 1 09 , 1 15 ],Memory based Learning (Sakkiset al.,2 00 0 b) [ 1 17 ], Support vector machines (Druker et al., 1 99 9) [ 3 6 ], Decision Trees (Carreras and Marquez, 2 00 1) [ 1 9 ], Maximum Entropy Model (Berger et al. 1 99 6) [ 1 4 ], Hash For...","[ 3 6 ] | [ 3 6 ] | (Druker et al., 1 99 9) [ 3 6 ] | Support vector machines
(Druker et al., 1 99 9) [ 3 6 ]",Format_3 6_Pattern_1 | Format_3 6_Pattern_2 | Format_3 6_Pattern_5 | Format_3 6_Pattern_6,"38, 38, 38, 38"
37,"Dunlavy DM, O ’ Leary DP, Conroy JM, Schlesinger JD (2 00 7) QCS: A system for querying, clustering",3,59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani and Maybury 1 99 9 [ 7 4...,59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani and Maybury 1 99 9 [ 7 4...,jic et al. 2 00 8 [ 1 59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani an...,[ 3 7 ] | [ 3 7 ] | Dunlavy et al. 2 00 7 [ 3 7 ],Format_3 7_Pattern_1 | Format_3 7_Pattern_2 | Format_3 7_Pattern_6,"42, 42, 42"
38,Elkan C (2 00 8) Log-Linear Models and Conditional Random Fields. http://cseweb.ucsd.edu/welkan/2 50 B/,3,"between languages. Whereas generative models can become troublesome when many features are used and discriminative models allow use of more features [ 3 8 ]. Few of the examples of discriminative methods are Logistic regression and conditional random fields (CRFs), generative methods are Naive Bayes...","between languages. Whereas generative models can become troublesome when many features are used and discriminative models allow use of more features [ 3 8 ]. Few of the examples of discriminative methods are Logistic regression and conditional random fields (CRFs), generative methods are Naive Bayes...",with a resemblance that is used to spot an unknown speaker ’ s language and would bid the deep knowledge of numerous languages to perform the match. Discriminative methods rely on a less knowledge-intensive approach and using distinction between languages. Whereas generative models can become troubl...,"[ 3 8 ] | [ 3 8 ] | Discriminative methods rely on a less knowledge-intensive approach and using
distinction between languages. Whereas generative models can become troublesome when
many features are used and discriminative models allow use of more features [ 3 8 ]",Format_3 8_Pattern_1 | Format_3 8_Pattern_2 | Format_3 8_Pattern_6,"58, 58, 58"
39,"Emele MC, Dorna M (1 99 8) Ambiguity preserving machine translation using packed representations. In",1,"biguity is preserving ambiguity, e.g. (Shemtov 1 99 7; Emele & Dorna 1 99 8; Knight & Langkilde 2 00 0; Tong Gao et al. 2 01 5, Umber & Bajwa 2 01 1) [ 3 9 , 4 6 , 6 5 , 1 25 , 1 39 ]. Their objectives are closely in line with removal or minimizing ambiguity. They cover a wide range of ambiguities a...",,,"[ 3 9 , 4 6 , 6 5 , 1 25 , 1 39 ]",Format_3 9_Pattern_2,22
40,"Europarl: A Parallel Corpus for Statistical Machine Translation (2 00 5) Philipp Koehn , MT Summit 2 00 5",3,"of many sentences. b) The Europarl parallel corpus is derived from the European Parliament ’ s proceedings. It is available in 2 1 European languages [ 4 0 ]. c) WMT 14 provides machine translation pairs for English-German and English-French. Separately, these datasets comprise 4.5 million and 3 5 m...","of many sentences. b) The Europarl parallel corpus is derived from the European Parliament ’ s proceedings. It is available in 2 1 European languages [ 4 0 ]. c) WMT 14 provides machine translation pairs for English-German and English-French. Separately, these datasets comprise 4.5 million and 3 5 m...",elopment Programme for Indian Languages (TDIL) launched its own data distribution portal ( www. tdil-dc.in ) which has cataloged datasets [ 2 4 ]. 3. Machine Translation: The task of converting the text of one natural language into another language while keeping the sense of the input text is known ...,"[ 4 0 ] | [ 4 0 ] | Machine Translation: The task of converting the text of one natural language into another
language while keeping the sense of the input text is known as machine translation.
Majorly used datasets are as follows:
a)
Tatoeba is a collection of multilingual sentence pairings. A tab-delimited pair of an
English text sequence and the translated French text sequence appears on each line of
the dataset. Each text sequence might be as simple as a single sentence or as complex as a
paragraph of many sentences.
b)
The Europarl parallel corpus is derived from the European Parliament ’ s proceedings. It is
available in 2 1 European languages [ 4 0 ]",Format_4 0_Pattern_1 | Format_4 0_Pattern_2 | Format_4 0_Pattern_6,"53, 53, 53"
41,"Fan Y, Tian F, Xia Y, Qin T, Li XY, Liu TY (2 02 0) Searching better architectures for neural machine",3,dy (BLEU) scores compared to various neural machine translation systems. It outperformed the commonly used MT system on a WMT 1 4 dataset. Fan et al. [ 4 1 ] introduced a gradient-based neural architecture search algorithm that automatically finds architecture with better performance than a transfor...,dy (BLEU) scores compared to various neural machine translation systems. It outperformed the commonly used MT system on a WMT 1 4 dataset. Fan et al. [ 4 1 ] introduced a gradient-based neural architecture search algorithm that automatically finds architecture with better performance than a transfor...,of up to 2.8 bi-lingual evaluation understudy (BLEU) scores compared to various neural machine translation systems. It outperformed the commonly used MT system on a WMT 1 4 dataset. Fan et al. [ 4 1 ] introduced a gradient-based neural architecture search algorithm that automatically finds architect...,"[ 4 1 ] | [ 4 1 ] | BLEU) scores compared to various neural
machine translation systems. It outperformed the commonly used MT system on a WMT 1 4
dataset.
Fan et al. [ 4 1 ]",Format_4 1_Pattern_1 | Format_4 1_Pattern_2 | Format_4 1_Pattern_6,"57, 57, 57"
42,"Fang H, Lu W, Wu F, Zhang Y, Shang X, Shao J, Zhuang Y (2 01 5) Topic aspect-oriented summarization",3,resent entity. Various topics can have various aspects and various preferences of features are used to represent various aspects. (Fang et al. 2 01 5 [ 4 2 ]) f) Dialogue System Dialogue systems are very prominent in real world applications ranging from providing support to performing a particular a...,resent entity. Various topics can have various aspects and various preferences of features are used to represent various aspects. (Fang et al. 2 01 5 [ 4 2 ]) f) Dialogue System Dialogue systems are very prominent in real world applications ranging from providing support to performing a particular a...,utilizes document-term and sentence term matrices. This approach groups and summarizes the documents simultaneously. (Wang et al. 2 01 1) [ 1 47 ]) – Topic Aspect-Oriented Summarization (TAOS) is based on topic factors. These topic factors are various features that describe topics such as capital wo...,"[ 4 2 ] | [ 4 2 ] | Topic Aspect-Oriented Summarization (TAOS) is based on topic factors. These topic
factors are various features that describe topics such as capital words are used to represent
entity. Various topics can have various aspects and various preferences of features are
used to represent various aspects. (Fang et al. 2 01 5 [ 4 2 ]",Format_4 2_Pattern_1 | Format_4 2_Pattern_2 | Format_4 2_Pattern_6,"42, 42, 42"
43,"Fattah MA, Ren F (2 00 9) GA, MR, FFNN, PNN and GMM based models for automatic text summari-",6,he two important categories are single document summarization and multi document summari- zation (Zajic et al. 2 00 8 [ 1 59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]...,]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani and Maybury 1 99 9 [ 7 4 ]; Fattah and Ren 2 00 9 [ 4 3 ]; Riedhammer et al. 2 01 0 [ 1 10 ]). Training data is required in a supervised system for selecting relevant material from the documents. Large...,he two important categories are single document summarization and multi document summari- zation (Zajic et al. 2 00 8 [ 1 59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]...,[ 4 3 ] | [ 4 3 ] | [ 4 3 ] | [ 4 3 ] | Fattah and Ren 2 00 9 [ 4 3 ] | Fattah and Ren 2 00 9 [ 4 3 ],Format_4 3_Pattern_1 | Format_4 3_Pattern_1 | Format_4 3_Pattern_2 | Format_4 3_Pattern_2 | Format_4 3_Pattern_6 | Format_4 3_Pattern_6,"42, 42, 42, 42, 42, 42"
44,Feldman S (1 99 9) NLP meets the jabberwocky: natural language processing in information retrieval.,3,"hna ” . That is why, to get the proper meaning of the sentence, the appropriate interpre- tation is considered by looking at the rest of the sentence [ 4 4 ]. f) Discourse While syntax and semantics level deal with sentence-length units, the discourse level of NLP deals with more than one sentence. ...","hna ” . That is why, to get the proper meaning of the sentence, the appropriate interpre- tation is considered by looking at the rest of the sentence [ 4 4 ]. f) Discourse While syntax and semantics level deal with sentence-length units, the discourse level of NLP deals with more than one sentence. ...","script ” . This level of processing also incorporates the semantic disambiguation of words with multiple senses (Elizabeth D. Liddy, 2 00 1) [ 6 8 ]. For example, the word “ bark ” as a noun can mean either as a sound that a dog makes or outer covering of the tree. The semantic level examines words ...","[ 4 4 ] | [ 4 4 ] | For
example, the word “ bark ” as a noun can mean either as a sound that a dog makes or outer
covering of the tree. The semantic level examines words for their dictionary interpretation or
interpretation is derived from the context of the sentence. For example: the sentence “ Krishna
is good and noble. ” This sentence is either talking about Lord Krishna or about a person
“ Krishna ” . That is why, to get the proper meaning of the sentence, the appropriate interpre-
tation is considered by looking at the rest of the sentence [ 4 4 ]",Format_4 4_Pattern_1 | Format_4 4_Pattern_2 | Format_4 4_Pattern_6,"18, 18, 18"
45,"Friedman C, Cimino JJ, Johnson SB (1 99 3) A conceptual model for clinical radiology reports. In",3,and Encoding System) that identifies clinical information in narrative reports and transforms the textual information into structured representation [ 4 5 ]. ### 3.3 NLP in talk,and Encoding System) that identifies clinical information in narrative reports and transforms the textual information into structured representation [ 4 5 ]. ### 3.3 NLP in talk,"analyze/comprehend medical sentences, and to preserve a knowledge of free text into a language independent knowledge representation [ 1 07 , 1 08 ]. The Columbia university of New York has developed an NLP system called MEDLEE (MEDical Language Extraction and Encoding System) that identifies clinica...","[ 4 5 ] | [ 4 5 ] | The Columbia university of New York has developed an
NLP system called MEDLEE (MEDical Language Extraction and Encoding System) that
identifies clinical information in narrative reports and transforms the textual information into
structured representation [ 4 5 ]",Format_4 5_Pattern_1 | Format_4 5_Pattern_2 | Format_4 5_Pattern_6,"44, 44, 44"
46,"Gao T, Dontcheva M, Adar E, Liu Z, Karahalios K Data Tone: managing ambiguity in natural language",1,"biguity is preserving ambiguity, e.g. (Shemtov 1 99 7; Emele & Dorna 1 99 8; Knight & Langkilde 2 00 0; Tong Gao et al. 2 01 5, Umber & Bajwa 2 01 1) [ 3 9 , 4 6 , 6 5 , 1 25 , 1 39 ]. Their objectives are closely in line with removal or minimizing ambiguity. They cover a wide range of ambiguities a...",,,"[ 3 9 , 4 6 , 6 5 , 1 25 , 1 39 ]",Format_4 6_Pattern_2,22
47,"Ghosh S, Vinyals O, Strope B, Roy S, Dean T, Heck L (2 01 6) Contextual lstm (clstm) models for large",2,"as word prediction, and sentence topic prediction. [ 4 7 ] In order to observe the word arrange- ment in forward and backward direction, bi-directional LSTM is explored by researchers [ 5 9 ]. In case of mach","as word prediction, and sentence topic prediction. [ 4 7 ] In order to observe the word arrange- ment in forward and backward direction, bi-directional LSTM is explored by researchers [ 5 9 ]. In case of mach",,[ 4 7 ] | [ 4 7 ],Format_4 7_Pattern_1 | Format_4 7_Pattern_2,"62, 62"
48,"Glasgow B, Mandell A, Binney D, Ghemri L, Fisher D (1 99 8) MITA: an information-extraction approach",4,open-ended questionnaires in the field of advertising. There is a system called MITA (Metlife ’ s Intelligent Text Analyzer) (Glasgow et al. (1 99 8) [ 4 8 ]) that extracts information from life insurance applications. Ahonen et al. (1 99 8) [ 1 ] suggested a mainstream framework for text mining tha...,open-ended questionnaires in the field of advertising. There is a system called MITA (Metlife ’ s Intelligent Text Analyzer) (Glasgow et al. (1 99 8) [ 4 8 ]) that extracts information from life insurance applications. Ahonen et al. (1 99 8) [ 1 ] suggested a mainstream framework for text mining tha...,consists of responses to open-ended questionnaires in the field of advertising. There is a system called MITA (Metlife ’ s Intelligent Text Analyzer) (Glasgow et al. (1 99 8) [ 4 8 ]) that extracts information from life insurance applications. Ahonen et al. (1 99 8) [ 1 ] suggested a mainstream fram...,"[ 4 8 ] | [ 4 8 ] | (Glasgow et al. (1 99 8)
[ 4 8 ] | There is a system called MITA (Metlife ’ s Intelligent Text Analyzer) (Glasgow et al. (1 99 8)
[ 4 8 ]",Format_4 8_Pattern_1 | Format_4 8_Pattern_2 | Format_4 8_Pattern_5 | Format_4 8_Pattern_6,"40, 40, 40, 40"
49,Goldberg Y (2 01 7) Neural network methods for natural language processing. Synthesis lectures on human,3,"rieval, text summarization, text classification, machine transla- tion, sentiment analysis and speech recognition. Initially focus was on feedforward [ 4 9 ] and CNN (convolutional neural network) architecture [ 6 9 ] but later researchers adopted recurrent neural networks to capture the context of ...","rieval, text summarization, text classification, machine transla- tion, sentiment analysis and speech recognition. Initially focus was on feedforward [ 4 9 ] and CNN (convolutional neural network) architecture [ 6 9 ] but later researchers adopted recurrent neural networks to capture the context of ...","d for a variety of NLP applications, including word prediction, sentence production, quality assurance, and intrusion detection systems [ 1 33 ]. (c) Neural Network Earlier machine learning techniques such as Naïve Bayes, HMM etc. were majorly used for NLP but by the end of 2 01 0, neural networks t...","[ 4 9 ] | [ 4 9 ] | Neural Network
Earlier machine learning techniques such as Naïve Bayes, HMM etc. were majorly used for
NLP but by the end of 2 01 0, neural networks transformed and enhanced NLP tasks by learning
multilevel features. Major use of neural networks in NLP is observed for word embedding
where words are represented in the form of vectors. These vectors can be used to recognize
similar words by observing their closeness in this vector space, other uses of neural networks
are observed in information retrieval, text summarization, text classification, machine transla-
tion, sentiment analysis and speech recognition. Initially focus was on feedforward [ 4 9 ]",Format_4 9_Pattern_1 | Format_4 9_Pattern_2 | Format_4 9_Pattern_6,"60, 60, 60"
50,"Gong Y, Liu X (2 00 1) Generic text summarization using relevance measure and latent semantic analysis.",3,ation (Zajic et al. 2 00 8 [ 1 59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised...,ation (Zajic et al. 2 00 8 [ 1 59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised...,portant categories are single document summarization and multi document summari- zation (Zajic et al. 2 00 8 [ 1 59 ]; Fattah and Ren 2 00 9 [ 4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang ...,"[ 5 0 ] | [ 5 0 ] | Summaries can also be of two types:
generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]",Format_5 0_Pattern_1 | Format_5 0_Pattern_2 | Format_5 0_Pattern_6,"42, 42, 42"
51,"Green Jr, BF, Wolf AK, Chomsky C, Laughery K (1 96 1) Baseball: an automatic question-answerer. In",4,"linguistic studies had also started. As early as 1 96 0, signature work influenced by AI began, with the BASEBALL Q-A systems (Green et al., 1 96 1) [ 5 1 ]. LUNAR (Woods,1 97 8) [ 1 52 ] and Winograd SHRDLU were natural successors of these systems, but they were seen as stepped-up sophistication, i...","linguistic studies had also started. As early as 1 96 0, signature work influenced by AI began, with the BASEBALL Q-A systems (Green et al., 1 96 1) [ 5 1 ]. LUNAR (Woods,1 97 8) [ 1 52 ] and Winograd SHRDLU were natural successors of these systems, but they were seen as stepped-up sophistication, i...","puters for literary and linguistic studies had also started. As early as 1 96 0, signature work influenced by AI began, with the BASEBALL Q-A systems (Green et al., 1 96 1) [ 5 1 ]. LUNAR (Woods,1 97 8) [ 1 52 ] and Winograd SHRDLU were natural successors of these systems, but they were seen as step...","[ 5 1 ] | [ 5 1 ] | (Green et al., 1 96 1) [ 5 1 ] | By this time, work on the use of computers for literary and linguistic
studies had also started. As early as 1 96 0, signature work influenced by AI began, with the
BASEBALL Q-A systems (Green et al., 1 96 1) [ 5 1 ]",Format_5 1_Pattern_1 | Format_5 1_Pattern_2 | Format_5 1_Pattern_5 | Format_5 1_Pattern_6,"26, 26, 26, 26"
52,"Greff K, Srivastava RK, Koutník J, Steunebrink BR, Schmidhuber J (2 01 6) LSTM: A search space",1,"eful in the cases where only the desired important information needs to be retained for a much longer time discarding the irrelevant information, see [ 5 2 , 5 8 ]. Further development in the LSTM has also led to a slightly simpler variant, called the gated recurrent unit (GRU), which has shown bett...",,,"[ 5 2 , 5 8 ]",Format_5 2_Pattern_2,31
53,"Grishman R, Sager N, Raze C, Bookchin B (1 97 3) The linguistic string parser. In proceedings of the",1,"pplied in the field as well. The Linguistic String Project-Medical Language Processor is one the large scale projects of NLP in the field of medicine [ 2 1 , 5 3 , 5 7 , 7 1 , 1 14 ]. The LSP- MLP helps enabling physicians to extract and summarize information of any signs or symptoms, drug dosage an...",,,"[ 2 1 , 5 3 , 5 7 , 7 1 , 1 14 ]",Format_5 3_Pattern_2,42
54,"Hayes PJ (1 99 2) Intelligent high-volume text processing using shallow, domain-specific techniques. Text-",8,"market data, newswires etc. and assign them to predefined categories or indices. For example, The Carnegie Group ’ s Construe system (Hayes, 1 99 1) [ 5 4 ], inputs Reuters articles and saves much time by doing the work that is to be done by staff or human indexers. Some companies have been using ca...","market data, newswires etc. and assign them to predefined categories or indices. For example, The Carnegie Group ’ s Construe system (Hayes, 1 99 1) [ 5 4 ], inputs Reuters articles and saves much time by doing the work that is to be done by staff or human indexers. Some companies have been using ca...","asualty reports, market data, newswires etc. and assign them to predefined categories or indices. For example, The Carnegie Group ’ s Construe system (Hayes, 1 99 1) [ 5 4 ], inputs Reuters articles and saves much time by doing the work that is to be done by staff or human indexers. Some companies h...","[ 5 4 ] | [ 5 4 ] | (Hayes, 1 99 1) [ 5 4 ] | Text Categorization
Categorization systems input a large flow of data like official documents, military casualty
reports, market data, newswires etc. and assign them to predefined categories or indices. For
example, The Carnegie Group ’ s Construe system (Hayes, 1 99 1) [ 5 4 ] | [ 5 4 ] | [ 5 4 ] | (Hayes, 1 99 2) [ 5 4 ] | Former one has higher accuracy but higher cost of
implementation while latter has lower implementation cost and is usually insufficient for
IR). Compound or Statistical Phrases (Compounds and statistical phrases index multi token
units instead of single tokens.) Word Sense Disambiguation (Word sense disambiguation is the
task of understanding the correct sense of a word in context. When used for information
retrieval, terms are replaced by their senses in the document vector.)
The extracted information can be applied for a variety of purposes, for example to prepare a
summary, to build databases, identify keywords, classifying text items according to some pre-
defined categories etc. For example, CONSTRUE, it was developed for Reuters, that is used in
classifying news stories (Hayes, 1 99 2) [ 5 4 ]",Format_5 4_Pattern_1 | Format_5 4_Pattern_2 | Format_5 4_Pattern_5 | Format_5 4_Pattern_6 | Format_5 4_Pattern_1 | Format_5 4_Pattern_2 | Format_5 4_Pattern_5 | Format_5 4_Pattern_6,"38, 38, 38, 38, 40, 40, 40, 40"
55,"Hendrix GG, Sacerdoti ED, Sagalowicz D, Slocum J (1 97 8) Developing a natural language interface to",4,"ject (Lea, 1 98 0) and other in some major system developments projects building database front ends. The front-end projects (Hendrix et al., 1 97 8) [ 5 5 ] were intended to go beyond LUNAR in interfacing the large databases. In early 1 98 0 s computational grammar theory became a very active area ...","ject (Lea, 1 98 0) and other in some major system developments projects building database front ends. The front-end projects (Hendrix et al., 1 97 8) [ 5 5 ] were intended to go beyond LUNAR in interfacing the large databases. In early 1 98 0 s computational grammar theory became a very active area ...","anding Research (SUR) project (Lea, 1 98 0) and other in some major system developments projects building database front ends. The front-end projects (Hendrix et al., 1 97 8) [ 5 5 ] were intended to go beyond LUNAR in interfacing the large databases. In early 1 98 0 s computational grammar theory b...","[ 5 5 ] | [ 5 5 ] | (Hendrix
et al., 1 97 8) [ 5 5 ] | Winograd SHRDLU were natural successors of these systems, but they were seen as
stepped-up sophistication, in terms of their linguistic and their task processing capabilities.
There was a widespread belief that progress could only be made on the two sides, one is
ARPA Speech Understanding Research (SUR) project (Lea, 1 98 0) and other in some major
system developments projects building database front ends. The front-end projects (Hendrix
et al., 1 97 8) [ 5 5 ]",Format_5 5_Pattern_1 | Format_5 5_Pattern_2 | Format_5 5_Pattern_5 | Format_5 5_Pattern_6,"26, 26, 26, 26"
57,"Hirschman L, Grishman R, Sager N (1 97 6) From text to structured information: automatic processing of",1,"pplied in the field as well. The Linguistic String Project-Medical Language Processor is one the large scale projects of NLP in the field of medicine [ 2 1 , 5 3 , 5 7 , 7 1 , 1 14 ]. The LSP- MLP helps enabling physicians to extract and summarize information of any signs or symptoms, drug dosage an...",,,"[ 2 1 , 5 3 , 5 7 , 7 1 , 1 14 ]",Format_5 7_Pattern_2,42
58,"Hochreiter S, Schmidhuber J (1 99 7) Long short-term memory. Neural Comput 9(8):1 73 5 – 1 78 0",1,"eful in the cases where only the desired important information needs to be retained for a much longer time discarding the irrelevant information, see [ 5 2 , 5 8 ]. Further development in the LSTM has also led to a slightly simpler variant, called the gated recurrent unit (GRU), which has shown bett...",,,"[ 5 2 , 5 8 ]",Format_5 8_Pattern_2,31
59,"Huang Z, Xu W, Yu K (2 01 5) Bidirectional LSTM-CRF models for sequence tagging. ar Xiv preprint",3,"opic prediction. [ 4 7 ] In order to observe the word arrange- ment in forward and backward direction, bi-directional LSTM is explored by researchers [ 5 9 ]. In case of machine translation, encoder-decoder architecture is used where dimensionality of input and output vector is not known. Neural net...","opic prediction. [ 4 7 ] In order to observe the word arrange- ment in forward and backward direction, bi-directional LSTM is explored by researchers [ 5 9 ]. In case of machine translation, encoder-decoder architecture is used where dimensionality of input and output vector is not known. Neural net...","as word prediction, and sentence topic prediction. [ 4 7 ] In order to observe the word arrange- ment in forward and backward direction, bi-directional LSTM is explored by researchers [ 5 9 ]. In case of machine translation, encoder-decoder architecture is used where dimensionality of input and outp...","[ 5 9 ] | [ 5 9 ] | In order to observe the word arrange-
ment in forward and backward direction, bi-directional LSTM is explored by researchers [ 5 9 ]",Format_5 9_Pattern_1 | Format_5 9_Pattern_2 | Format_5 9_Pattern_6,"62, 62, 62"
60,"Hutchins WJ (1 98 6) Machine translation: past, present, future (p. 6 6). Ellis Horwood, Chichester",4,"C report, which concluded that MT is going nowhere. But later, some MT production systems were providing output to their customers (Hutchins, 1 98 6) [ 6 0 ]. By this time, work on the use of computers for literary and linguistic studies had also started. As early as 1 96 0, signature work influence...","C report, which concluded that MT is going nowhere. But later, some MT production systems were providing output to their customers (Hutchins, 1 98 6) [ 6 0 ]. By this time, work on the use of computers for literary and linguistic studies had also started. As early as 1 96 0, signature work influence...","cording to the ALPAC report, which concluded that MT is going nowhere. But later, some MT production systems were providing output to their customers (Hutchins, 1 98 6) [ 6 0 ]. By this time, work on the use of computers for literary and linguistic studies had also started. As early as 1 96 0, signa...","[ 6 0 ] | [ 6 0 ] | (Hutchins, 1 98 6) [ 6 0 ] | In fact, MT/NLP research
almost died in 1 96 6 according to the ALPAC report, which concluded that MT is going
nowhere. But later, some MT production systems were providing output to their customers
(Hutchins, 1 98 6) [ 6 0 ]",Format_6 0_Pattern_1 | Format_6 0_Pattern_2 | Format_6 0_Pattern_5 | Format_6 0_Pattern_6,"26, 26, 26, 26"
61,"Jurafsky D, Martin J (2 00 8) H. Speech and language processing. 2 nd edn. Prentice-Hall, Englewood",4,t is also explored in unusual areas like segmentation for infant learning and identifying documents for opinions and facts. Anggraeni et al. (2 01 9) [ 6 1 ] used ML and AI to create a question-and-answer system for retrieving information about hearing loss. They developed I-Chat Bot which understan...,t is also explored in unusual areas like segmentation for infant learning and identifying documents for opinions and facts. Anggraeni et al. (2 01 9) [ 6 1 ] used ML and AI to create a question-and-answer system for retrieving information about hearing loss. They developed I-Chat Bot which understan...,ion but it is also explored in unusual areas like segmentation for infant learning and identifying documents for opinions and facts. Anggraeni et al. (2 01 9) [ 6 1 ] used ML and AI to create a question-and-answer system for retrieving information about hearing loss. They developed I-Chat Bot which ...,"[ 6 1 ] | [ 6 1 ] | (2 01 9) [ 6 1 ] | Naive Bayes is a probabilistic algorithm which is based on probability theory and Bayes ’
Theorem to predict the tag of a text such as news or customer review. It helps to calculate the
probability of each tag for the given text and return the tag with the highest probability. Bayes ’
Theorem is used to predict the probability of a feature based on prior knowledge of conditions
that might be related to that feature. The choice of area in NLP using Naïve Bayes Classifiers
could be in usual tasks such as segmentation and translation but it is also explored in unusual
areas like segmentation for infant learning and identifying documents for opinions and facts.
Anggraeni et al. (2 01 9) [ 6 1 ]",Format_6 1_Pattern_1 | Format_6 1_Pattern_2 | Format_6 1_Pattern_5 | Format_6 1_Pattern_6,"60, 60, 60, 60"
62,"Kamp H, Reyle U (1 99 3) Tense and aspect. In from discourse to logic (pp. 4 83-6 89). Springer Netherlands",4,"eral purpose sentence processors like SRI ’ s Core Language Engine (Alshawi,1 99 2) [ 2 ] and Discourse Representation Theory (Kamp and Reyle,1 99 3) [ 6 2 ] offered a means of tackling more extended discourse within the grammatico-logical framework. This period was one of the growing communities. P...","eral purpose sentence processors like SRI ’ s Core Language Engine (Alshawi,1 99 2) [ 2 ] and Discourse Representation Theory (Kamp and Reyle,1 99 3) [ 6 2 ] offered a means of tackling more extended discourse within the grammatico-logical framework. This period was one of the growing communities. P...","decade the powerful general purpose sentence processors like SRI ’ s Core Language Engine (Alshawi,1 99 2) [ 2 ] and Discourse Representation Theory (Kamp and Reyle,1 99 3) [ 6 2 ] offered a means of tackling more extended discourse within the grammatico-logical framework. This period was one of the...","[ 6 2 ] | [ 6 2 ] | (Kamp and
Reyle,1 99 3) [ 6 2 ] | Discourse Representation Theory (Kamp and
Reyle,1 99 3) [ 6 2 ]",Format_6 2_Pattern_1 | Format_6 2_Pattern_2 | Format_6 2_Pattern_5 | Format_6 2_Pattern_6,"26, 26, 26, 26"
63,"Kang Y, Cai Z, Tan CW, Huang Q, Liu H (2 02 0) Natural language processing (NLP) in management",5,"nd challenges, Baclic et.al. (2 02 0) [ 6 ] and Wong et al. (2 01 8) [ 1 51 ] for challenges and opportunities in public health, Kang et.al. (2 02 0) [ 6 3 ] for detailed literature survey and technological challenges relevant to management research and NLP, and a recent review work by Alshemali and...","nd challenges, Baclic et.al. (2 02 0) [ 6 ] and Wong et al. (2 01 8) [ 1 51 ] for challenges and opportunities in public health, Kang et.al. (2 02 0) [ 6 3 ] for detailed literature survey and technological challenges relevant to management research and NLP, and a recent review work by Alshemali and...","egories and challenges, Baclic et.al. (2 02 0) [ 6 ] and Wong et al. (2 01 8) [ 1 51 ] for challenges and opportunities in public health, Kang et.al. (2 02 0) [ 6 3 ] for detailed literature survey and technological challenges relevant to management research and NLP, and a recent review work by Alsh...","[ 6 3 ] | [ 6 3 ] | (2 02 0) [ 6 3 ] | Kang et.al.
(2 02 0) [ 6 3 ] | [ 1 5 , 3 2 , 6 3 , 9 8 , 1 33 , 1 51 ]",Format_6 3_Pattern_1 | Format_6 3_Pattern_2 | Format_6 3_Pattern_5 | Format_6 3_Pattern_6 | Format_6 3_Pattern_2,"70, 70, 70, 70, 73"
64,Kim Y. (2 01 4) Convolutional neural networks for sentence classification. ar Xiv preprint ar Xiv:1 40 8.5 88 2,4,"eacher who responds by offering brief extracts from the text. The neural learning models are overtaking traditional models for NLP [ 6 4 , 1 27 ]. In [ 6 4 ], authors used CNN (Convolutional Neural Network) model for sentiment analysis of movie reviews and achieved 8 1.5% accuracy. The results illus...","pedia text, and a teacher who responds by offering brief extracts from the text. The neural learning models are overtaking traditional models for NLP [ 6 4 , 1 27 ]. In [ 6 4 ], authors used CNN (Convolutional Neural Network) model for sentiment analysis of movie reviews and achieved 8 1.5% accuracy...","eacher who responds by offering brief extracts from the text. The neural learning models are overtaking traditional models for NLP [ 6 4 , 1 27 ]. In [ 6 4 ], authors used CNN (Convolutional Neural Network) model for sentiment analysis of movie reviews and achieved 8 1.5% accuracy. The results illus...","[ 6 4 ] | [ 6 4 , 1 27 ] | [ 6 4 ] | In [ 6 4 ]",Format_6 4_Pattern_1 | Format_6 4_Pattern_2 | Format_6 4_Pattern_2 | Format_6 4_Pattern_6,"55, 55, 55, 55"
65,"Knight K, Langkilde I (2 00 0) Preserving ambiguities in generation via automata intersection. In AAAI/",1,"biguity is preserving ambiguity, e.g. (Shemtov 1 99 7; Emele & Dorna 1 99 8; Knight & Langkilde 2 00 0; Tong Gao et al. 2 01 5, Umber & Bajwa 2 01 1) [ 3 9 , 4 6 , 6 5 , 1 25 , 1 39 ]. Their objectives are closely in line with removal or minimizing ambiguity. They cover a wide range of ambiguities a...",,,"[ 3 9 , 4 6 , 6 5 , 1 25 , 1 39 ]",Format_6 5_Pattern_2,22
66,"Lass R (1 99 8) Phonology: An Introduction to Basic Concepts. Cambridge, UK; New York; Melbourne,",3,"Phonology is “ the study of sound pertaining to the system of language ” whereas Lass 19 98 [ 6 6 ]wrote that phonology refers broadly with the sounds of language, concerned with sub- discipline of linguistics, behavior and organization of sounds. P","Phonology is “ the study of sound pertaining to the system of language ” whereas Lass 19 98 [ 6 6 ]wrote that phonology refers broadly with the sounds of language, concerned with sub- discipline of linguistics, behavior and organization of sounds. P","Phonology is “ the study of sound pertaining to the system of language ” whereas Lass 19 98 [ 6 6 ]wrote that phonology refers broadly with the sounds of language, concerned with sub- discipline of linguistics, behavior and organization of sounds. P","[ 6 6 ] | [ 6 6 ] | Phonology is “ the study of sound pertaining to the system of language ” whereas Lass 19 98
[ 6 6 ]",Format_6 6_Pattern_1 | Format_6 6_Pattern_2 | Format_6 6_Pattern_6,"16, 16, 16"
67,Lewis DD (1 99 8) Naive (Bayes) at forty: The independence assumption in information retrieval. In,4,"learned from training data rather than making by hand. The naïve bayes is preferred because of its performance despite its simplicity (Lewis, 1 99 8) [ 6 7 ] In Text Categorization two types of models have been used (Mc Callum and Nigam, 1 99 8) [ 7 7 ]. Both modules assume that a fixed vocabulary i...","learned from training data rather than making by hand. The naïve bayes is preferred because of its performance despite its simplicity (Lewis, 1 99 8) [ 6 7 ] In Text Categorization two types of models have been used (Mc Callum and Nigam, 1 99 8) [ 7 7 ]. Both modules assume that a fixed vocabulary i...","s classifier is learned from training data rather than making by hand. The naïve bayes is preferred because of its performance despite its simplicity (Lewis, 1 99 8) [ 6 7 ] In Text Categorization two types of models have been used (Mc Callum and Nigam, 1 99 8) [ 7 7 ]. Both modules assume that a fi...","[ 6 7 ] | [ 6 7 ] | (Lewis, 1 99 8) [ 6 7 ] | Using these
approaches is better as classifier is learned from training data rather than making by hand. The
naïve bayes is preferred because of its performance despite its simplicity (Lewis, 1 99 8) [ 6 7 ]",Format_6 7_Pattern_1 | Format_6 7_Pattern_2 | Format_6 7_Pattern_5 | Format_6 7_Pattern_6,"38, 38, 38, 38"
68,Liddy ED (2 00 1). Natural language processing,12,"e ” or “ script ” . This level of processing also incorporates the semantic disambiguation of words with multiple senses (Elizabeth D. Liddy, 2 00 1) [ 6 8 ]. For example, the word “ bark ” as a noun can mean either as a sound that a dog makes or outer covering of the tree. The semantic level examin...","that convey meaning by interpreting the relations between sentences and uncovering linguistic structures from texts at several levels (Liddy,2 00 1) [ 6 8 ]. The two of the most common levels are: Anaphora Resolution an d Coreference Resolution. Anaphora resolution 3 71 7 Multimedia Tools and Applic...","e ” or “ script ” . This level of processing also incorporates the semantic disambiguation of words with multiple senses (Elizabeth D. Liddy, 2 00 1) [ 6 8 ]. For example, the word “ bark ” as a noun can mean either as a sound that a dog makes or outer covering of the tree. The semantic level examin...","[ 6 8 ] | [ 6 8 ] | [ 6 8 ] | [ 6 8 ] | (Elizabeth D. Liddy, 2 00 1) [ 6 8 ] | (Liddy,2 00 1) [ 6 8 ] | It retains the stopwords as
removal of them changes the meaning of the sentence. It doesn ’ t support lemmatization and
stemming because converting words to its basic form changes the grammar of the sentence. It
focuses on identification on correct Po S of sentences. For example: in the sentence “ frowns on
his face ” , “ frowns ” is a noun whereas it is a verb in the sentence “ he frowns ” .
e)
Semantic
On a semantic level, the most important task is to determine the proper meaning of a sentence.
To understand the meaning of a sentence, human beings rely on the knowledge about language
and the concepts present in that sentence, but machines can ’ t count on these techniques.
Semantic processing determines the possible meanings of a sentence by processing its logical
structure to recognize the most relevant words to understand the interactions among words or
different concepts in the sentence. For example, it understands that a sentence is about
“ movies ” even if it doesn ’ t comprise actual words, but it contains related concepts such as
“ actor ” , “ actress ” , “ dialogue ” or “ script ” . This level of processing also incorporates the
semantic disambiguation of words with multiple senses (Elizabeth D. Liddy, 2 00 1) [ 6 8 ] | Discourse
While syntax and semantics level deal with sentence-length units, the discourse level of NLP
deals with more than one sentence. It deals with the analysis of logical structure by making
connections among words and sentences that ensure its coherence. It focuses on the properties
of the text that convey meaning by interpreting the relations between sentences and uncovering
linguistic structures from texts at several levels (Liddy,2 00 1) [ 6 8 ] | [ 6 8 ] | [ 6 8 ] | (Liddy, 2 00 1) [ 6 8 ] | Dialogue System
Dialogue systems are very prominent in real world applications ranging from providing
support to performing a particular action. In case of support dialogue systems, context
awareness is required whereas in case to perform an action, it doesn ’ t require much context
awareness. Earlier dialogue systems were focused on small applications such as home theater
systems. These dialogue systems utilize phonemic and lexical levels of language. Habitable
dialogue systems offer potential for fully automated dialog systems by utilizing all levels of a
language. (Liddy, 2 00 1) [ 6 8 ]",Format_6 8_Pattern_1 | Format_6 8_Pattern_1 | Format_6 8_Pattern_2 | Format_6 8_Pattern_2 | Format_6 8_Pattern_5 | Format_6 8_Pattern_5 | Format_6 8_Pattern_6 | Format_6 8_Pattern_6 | Format_6 8_Pattern_1 | Format_6 8_Pattern_2 | Format_6 8_Pattern_5 | Format_6 8_Pattern_6,"18, 18, 18, 18, 18, 18, 18, 18, 42, 42, 42, 42"
69,"Lopez MM, Kalita J (2 01 7) Deep learning applied to NLP. ar Xiv preprint ar Xiv:1 70 3.0 30 91",3,"nsla- tion, sentiment analysis and speech recognition. Initially focus was on feedforward [ 4 9 ] and CNN (convolutional neural network) architecture [ 6 9 ] but later researchers adopted recurrent neural networks to capture the context of a word with respect to surrounding words of a sentence. LSTM...","nsla- tion, sentiment analysis and speech recognition. Initially focus was on feedforward [ 4 9 ] and CNN (convolutional neural network) architecture [ 6 9 ] but later researchers adopted recurrent neural networks to capture the context of a word with respect to surrounding words of a sentence. LSTM...","summarization, text classification, machine transla- tion, sentiment analysis and speech recognition. Initially focus was on feedforward [ 4 9 ] and CNN (convolutional neural network) architecture [ 6 9 ] but later researchers adopted recurrent neural networks to capture the context of a word with r...",[ 6 9 ] | [ 6 9 ] | CNN (convolutional neural network) architecture [ 6 9 ],Format_6 9_Pattern_1 | Format_6 9_Pattern_2 | Format_6 9_Pattern_6,"60, 60, 60"
70,"Luong MT, Sutskever I, Le Q V, Vinyals O, Zaremba W (2 01 4) Addressing the rare word problem in",6,"ks like Sentence Classification [ 1 27 ], Sentiment Analysis [ 1 35 ], Text Classification [ 1 18 ], Text Summarization [ 1 58 ], Machine Translation [ 7 0 ] and Answer Relations [ 1 50 ] . An article by Newatia (2 01 9) [ 9 3 ] illustrates the general architecture behind any CNN model, and how it c...","ks like Sentence Classification [ 1 27 ], Sentiment Analysis [ 1 35 ], Text Classification [ 1 18 ], Text Summarization [ 1 58 ], Machine Translation [ 7 0 ] and Answer Relations [ 1 50 ] . An article by Newatia (2 01 9) [ 9 3 ] illustrates the general architecture behind any CNN model, and how it c...","ociated with NLP tasks like Sentence Classification [ 1 27 ], Sentiment Analysis [ 1 35 ], Text Classification [ 1 18 ], Text Summarization [ 1 58 ], Machine Translation [ 7 0 ] and Answer Relations [ 1 50 ] . An article by Newatia (2 01 9) [ 9 3 ] illustrates the general architecture behind any CNN...","[ 7 0 ] | [ 7 0 ] | Machine Translation
[ 7 0 ] | [ 7 0 ] | [ 7 0 ] | Quasi-
Recurrent Neural Network and LSTM to handle the granularity at character and word level.
They tuned the parameters for character-level modeling using Penn Treebank dataset and
word-level modeling using Wiki Text-1 03. In both cases, their model outshined the state-of-art
methods.
Luong et al. [ 7 0 ]",Format_7 0_Pattern_1 | Format_7 0_Pattern_2 | Format_7 0_Pattern_6 | Format_7 0_Pattern_1 | Format_7 0_Pattern_2 | Format_7 0_Pattern_6,"29, 29, 29, 55, 55, 55"
71,"Lyman M, Sager N, Friedman C, Chi E (1 98 5) Computer-structured narrative in ambulatory care: its use in",1,"pplied in the field as well. The Linguistic String Project-Medical Language Processor is one the large scale projects of NLP in the field of medicine [ 2 1 , 5 3 , 5 7 , 7 1 , 1 14 ]. The LSP- MLP helps enabling physicians to extract and summarize information of any signs or symptoms, drug dosage an...",,,"[ 2 1 , 5 3 , 5 7 , 7 1 , 1 14 ]",Format_7 1_Pattern_2,42
72,"Lyman M, Sager N, Chi EC, Tick LJ, Nhan NT, Su Y, ..., Scherrer, J. (1 98 9) Medical Language",1,"vironment with NLP features [ 8 1 , 1 19 ]. In the first phase, patient records were archived. At later stage the LSP-MLP has been adapted for French [ 1 0 , 7 2 , 9 4 , 1 13 ], and finally, a proper NLP system called RECIT [ 9 , 1 1 , 1 7 , 1 06 ] has been developed using a method called Proximity ...",,,"[ 1 0 , 7 2 , 9 4 , 1 13 ]",Format_7 2_Pattern_2,44
73,"Maas A, Daly RE, Pham PT, Huang D, Ng AY, Potts C (2 01 1) Learning word vectors for sentiment",3,"alysis, this dataset offers thousands of movie reviews split into training and test datasets. This dataset was introduced in by Mass et al. in 2 01 1 [ 7 3 ]. e) G.Rama Rohit Reddy of the Language Technologies Research Centre, KCIS, IIIT Hyderabad, generated the corpus “ Sentiraama. ” The corpus is ...","alysis, this dataset offers thousands of movie reviews split into training and test datasets. This dataset was introduced in by Mass et al. in 2 01 1 [ 7 3 ]. e) G.Rama Rohit Reddy of the Language Technologies Research Centre, KCIS, IIIT Hyderabad, generated the corpus “ Sentiraama. ” The corpus is ...","c) Paper Reviews: It provides reviews of computing and informatics conferences written in English and Spanish languages. It has 4 05 reviews which are evaluated on a 5-point scale ranging from very negative to very positive. d) IMDB: For natural language processing, text analytics, and sentiment ana...","[ 7 3 ] | [ 7 3 ] | Paper Reviews: It provides reviews of computing and informatics conferences written in
English and Spanish languages. It has 4 05 reviews which are evaluated on a 5-point scale
ranging from very negative to very positive.
d)
IMDB: For natural language processing, text analytics, and sentiment analysis, this
dataset offers thousands of movie reviews split into training and test datasets. This dataset
was introduced in by Mass et al. in 2 01 1 [ 7 3 ]",Format_7 3_Pattern_1 | Format_7 3_Pattern_2 | Format_7 3_Pattern_6,"53, 53, 53"
74,"Mani I, Maybury MT (eds) (1 99 9) Advances in automatic text summarization, vol 2 93. MIT press,",7,"nd Schuetze,1 99 9) [ 7 5 ], because this not only involves data analysts. Information extraction and automatic summarizing (Mani and Maybury,1 99 9) [ 7 4 ] was also a point of focus. Next, we present a walkthrough of the developments from the early 2 00 0. ### 3.1 A walkthrough of recent developme...","nd Schuetze,1 99 9) [ 7 5 ], because this not only involves data analysts. Information extraction and automatic summarizing (Mani and Maybury,1 99 9) [ 7 4 ] was also a point of focus. Next, we present a walkthrough of the developments from the early 2 00 0. ### 3.1 A walkthrough of recent developme...","thing in 9 0 s (Manning and Schuetze,1 99 9) [ 7 5 ], because this not only involves data analysts. Information extraction and automatic summarizing (Mani and Maybury,1 99 9) [ 7 4 ] was also a point of focus. Next, we present a walkthrough of the developments from the early 2 00 0. ### 3.1 A walkth...","[ 7 4 ] | [ 7 4 ] | (Mani and Maybury,1 99 9) [ 7 4 ] | Information extraction and automatic summarizing (Mani and Maybury,1 99 9) [ 7 4 ] | [ 7 4 ] | [ 7 4 ] | Summarization task can be either supervised or unsupervised (Mani
and Maybury 1 99 9 [ 7 4 ]",Format_7 4_Pattern_1 | Format_7 4_Pattern_2 | Format_7 4_Pattern_5 | Format_7 4_Pattern_6 | Format_7 4_Pattern_1 | Format_7 4_Pattern_2 | Format_7 4_Pattern_6,"28, 28, 28, 28, 42, 42, 42"
75,"Manning CD, Schütze H (1 99 9) Foundations of statistical natural language processing, vol 9 99. MIT",4,"LP, the work on the lexicon, also pointed in this direction. Statistical language processing was a major thing in 9 0 s (Manning and Schuetze,1 99 9) [ 7 5 ], because this not only involves data analysts. Information extraction and automatic summarizing (Mani and Maybury,1 99 9) [ 7 4 ] was also a p...","LP, the work on the lexicon, also pointed in this direction. Statistical language processing was a major thing in 9 0 s (Manning and Schuetze,1 99 9) [ 7 5 ], because this not only involves data analysts. Information extraction and automatic summarizing (Mani and Maybury,1 99 9) [ 7 4 ] was also a p...","works, statistically colored NLP, the work on the lexicon, also pointed in this direction. Statistical language processing was a major thing in 9 0 s (Manning and Schuetze,1 99 9) [ 7 5 ], because this not only involves data analysts. Information extraction and automatic summarizing (Mani and Maybur...","[ 7 5 ] | [ 7 5 ] | (Manning and Schuetze,1 99 9) [ 7 5 ] | NLP,
the work on the lexicon, also pointed in this direction. Statistical language processing was a
major thing in 9 0 s (Manning and Schuetze,1 99 9) [ 7 5 ]",Format_7 5_Pattern_1 | Format_7 5_Pattern_2 | Format_7 5_Pattern_5 | Format_7 5_Pattern_6,"28, 28, 28, 28"
76,"Marcus MP, Marcinkiewicz MA, Santorini B (1 99 3) Building a large annotated corpus of english: the",3,"7 3,0 00 tokens for validation, and 8 2,0 00 tokens for testing purposes. Its context is limited since it comprises sentences rather than paragraphs [ 7 6 ]. d) The Ministry of Electronics and Information Technology ’ s Technology Development Programme for Indian Languages (TDIL) launched its own da...","7 3,0 00 tokens for validation, and 8 2,0 00 tokens for testing purposes. Its context is limited since it comprises sentences rather than paragraphs [ 7 6 ]. d) The Ministry of Electronics and Information Technology ’ s Technology Development Programme for Indian Languages (TDIL) launched its own da...","nnotation technique for each of them. The folder “ Song Lyrics ” in the corpus contains 3 39 Telugu song lyrics written in Telugu script [ 1 21 ]. 2. Language Modelling: Language models analyse text data to calculate word probability. They use an algorithm to interpret the data, which establishes ru...","[ 7 6 ] | [ 7 6 ] | Language Modelling: Language models analyse text data to calculate word probability.
They use an algorithm to interpret the data, which establishes rules for context in natural
language. The model then uses these rules to accurately predict or construct new
sentences. The model basically learns the basic characteristics and features of language
and then applies them to new phrases. Majorly used datasets for Language modeling are
as follows:
a)
Salesforce ’ s Wiki Text-1 03 dataset has 1 03 million tokens collected from 2 8,4 75 featured
articles from Wikipedia.
b)
Wiki Text-2 is a scaled-down version of Wiki Text-1 03. It contains 2 million tokens with a
3 3,2 78 jargon size.
c)
Penn Treebank piece of the Wall Street Diary corpus includes 9 29,0 00 tokens for training,
7 3,0 00 tokens for validation, and 8 2,0 00 tokens for testing purposes. Its context is limited
since it comprises sentences rather than paragraphs [ 7 6 ]",Format_7 6_Pattern_1 | Format_7 6_Pattern_2 | Format_7 6_Pattern_6,"53, 53, 53"
77,"Mc Callum A, Nigam K (1 99 8) A comparison of event models for naive bayes text classification. In AAAI-",8,"ts performance despite its simplicity (Lewis, 1 99 8) [ 6 7 ] In Text Categorization two types of models have been used (Mc Callum and Nigam, 1 99 8) [ 7 7 ]. Both modules assume that a fixed vocabulary is present. But in first model a document is generated by first choosing a subset of vocabulary a...","ts performance despite its simplicity (Lewis, 1 99 8) [ 6 7 ] In Text Categorization two types of models have been used (Mc Callum and Nigam, 1 99 8) [ 7 7 ]. Both modules assume that a fixed vocabulary is present. But in first model a document is generated by first choosing a subset of vocabulary a...","ayes is preferred because of its performance despite its simplicity (Lewis, 1 99 8) [ 6 7 ] In Text Categorization two types of models have been used (Mc Callum and Nigam, 1 99 8) [ 7 7 ]. Both modules assume that a fixed vocabulary is present. But in first model a document is generated by first cho...","[ 7 7 ] | [ 7 7 ] | (Mc Callum and Nigam, 1 99 8) [ 7 7 ] | In
Text Categorization two types of models have been used (Mc Callum and Nigam, 1 99 8) [ 7 7 ] | [ 7 7 ] | [ 7 7 ] | (Mc Callum and Nigam, 1 99 8)
[ 7 7 ] | In the case of a
domain specific search engine, the automatic identification of important information can
increase accuracy and efficiency of a directed search. There is use of hidden Markov models
(HMMs) to extract the relevant fields of research papers. These extracted text segments are
used to allow searched over specific fields and to provide effective presentation of search
results and to match references to papers. For example, noticing the pop-up ads on any
websites showing the recent items you might have looked on an online store with discounts.
In Information Retrieval two types of models have been used (Mc Callum and Nigam, 1 99 8)
[ 7 7 ]",Format_7 7_Pattern_1 | Format_7 7_Pattern_2 | Format_7 7_Pattern_5 | Format_7 7_Pattern_6 | Format_7 7_Pattern_1 | Format_7 7_Pattern_2 | Format_7 7_Pattern_5 | Format_7 7_Pattern_6,"38, 38, 38, 38, 40, 40, 40, 40"
78,Mc Cray AT (1 99 1) Natural language processing for intelligent information retrieval. In Engineering in,0,NOT CITED,,,,,
79,Mc Cray AT (1 99 1) Extending a natural language parser with UMLS knowledge. In proceedings of the,0,NOT CITED,,,,,
80,"Mc Cray AT, Nelson SJ (1 99 5) The representation of meaning in the UMLS. Methods Inf Med 3 4(1 – 2):",0,NOT CITED,,,,,
81,"Mc Cray AT, Razi A (1 99 4) The UMLS knowledge source server. Medinfo Med Info 8:1 44 – 1 47",1,"aries. The Centre d ’ Informatique Hospitaliere of the Hopital Cantonal de Geneve is working on an electronic archiving environment with NLP features [ 8 1 , 1 19 ]. In the first phase, patient records were archived. At later stage the LSP-MLP has been adapted for French [ 1 0 , 7 2 , 9 4 , 1 13 ], ...",,,"[ 8 1 , 1 19 ]",Format_8 1_Pattern_2,44
82,"Mc Cray AT, Srinivasan S, Browne AC (1 99 4) Lexical methods for managing variation in biomedical",0,NOT CITED,,,,,
83,"Mc Donald R, Crammer K, Pereira F (2 00 5) Flexible text segmentation with structured multilabel",1,"g is often evaluated using the Co NLL 2 00 0 shared task. Various researchers (Sha and Pereira, 2 00 3; Mc Donald et al., 2 00 5; Sun et al., 2 00 8) [ 8 3 , 1 22 , 1 30 ] used Co NLL test data for chunking and used features composed of words, POS tags, and tags. There are particular words in the do...",,,"[ 8 3 ,
1 22 , 1 30 ]",Format_8 3_Pattern_2,33
84,"Mc Gray AT, Sponsler JL, Brylawski B, Browne AC (1 98 7) The role of lexical knowledge in biomedical",0,NOT CITED,,,,,
85,"Mc Keown KR (1 98 5) Text generation. Cambridge University Press, Cambridge",4,"r with phonological assumptions on which it is based and the evidence from which they have drawn their proposals. At the same time, Mc Keown (1 98 5) [ 8 5 ] demonstrated that rhetorical schemas could be used for producing both linguistically coherent and communica- tively effective text. Some resea...","r with phonological assumptions on which it is based and the evidence from which they have drawn their proposals. At the same time, Mc Keown (1 98 5) [ 8 5 ] demonstrated that rhetorical schemas could be used for producing both linguistically coherent and communica- tively effective text. Some resea...",", together with phonological assumptions on which it is based and the evidence from which they have drawn their proposals. At the same time, Mc Keown (1 98 5) [ 8 5 ] demonstrated that rhetorical schemas could be used for producing both linguistically coherent and communica- tively effective text. S...","[ 8 5 ] | [ 8 5 ] | (1 98 5) [ 8 5 ] | At the same time, Mc Keown (1 98 5) [ 8 5 ]",Format_8 5_Pattern_1 | Format_8 5_Pattern_2 | Format_8 5_Pattern_5 | Format_8 5_Pattern_6,"28, 28, 28, 28"
86,"Merity S, Keskar NS, Socher R (2 01 8) An analysis of neural language modeling at multiple scales. ar Xiv",3,oaches for dealing with relational reasoning on compartmentalized information. The results achieved with RMC show improved performance. Merity et al. [ 8 6 ] extended conventional word-level language models based on Quasi- Recurrent Neural Network and LSTM to handle the granularity at character and ...,oaches for dealing with relational reasoning on compartmentalized information. The results achieved with RMC show improved performance. Merity et al. [ 8 6 ] extended conventional word-level language models based on Quasi- Recurrent Neural Network and LSTM to handle the granularity at character and ...,"h the capacity to learn on classifying the information and perform complex reasoning based on the interactions between compartmentalized information. They used the relational memory core to handle such interactions. Finally, the model was tested for language modeling on three different datasets (Gig...","[ 8 6 ] | [ 8 6 ] | They used the relational memory core to handle
such interactions. Finally, the model was tested for language modeling on three different
datasets (Giga Word, Project Gutenberg, and Wiki Text-1 03). Further, they mapped the perfor-
mance of their model to traditional approaches for dealing with relational reasoning on
compartmentalized information. The results achieved with RMC show improved performance.
Merity et al. [ 8 6 ]",Format_8 6_Pattern_1 | Format_8 6_Pattern_2 | Format_8 6_Pattern_6,"55, 55, 55"
87,"Mikolov T, Chen K, Corrado G., & Dean, J. (2 01 3). Distributed representations of words and phrases and",3,"eld of NLP, where two convolutional models with max pooling were used to perform parts-of-speech and named entity recognition tagging. Mikolov et.al. [ 8 7 ] proposed a word embedding process where the dense vector representation of text was addressed. They also report the challenges faced by tradit...","eld of NLP, where two convolutional models with max pooling were used to perform parts-of-speech and named entity recognition tagging. Mikolov et.al. [ 8 7 ] proposed a word embedding process where the dense vector representation of text was addressed. They also report the challenges faced by tradit...","okup table which represents the n previous words in sequence. Collobert et al. [ 2 9 ] proposed the application of multitask learning in the field of NLP, where two convolutional models with max pooling were used to perform parts-of-speech and named entity recognition tagging. Mikolov et.al. [ 8 7 ]...","[ 8 7 ] | [ 8 7 ] | NLP, where
two convolutional models with max pooling were used to perform parts-of-speech and named
entity recognition tagging. Mikolov et.al. [ 8 7 ]",Format_8 7_Pattern_1 | Format_8 7_Pattern_2 | Format_8 7_Pattern_6,"29, 29, 29"
88,"Morel-Guillemaz AM, Baud RH, Scherrer JR (1 99 0) Proximity processing of medical text. In medical",3,", 9 4 , 1 13 ], and finally, a proper NLP system called RECIT [ 9 , 1 1 , 1 7 , 1 06 ] has been developed using a method called Proximity Processing [ 8 8 ]. It ’ s task was to implement a robust and multilingual system able to analyze/comprehend medical sentences, and to preserve a knowledge of fre...",", 9 4 , 1 13 ], and finally, a proper NLP system called RECIT [ 9 , 1 1 , 1 7 , 1 06 ] has been developed using a method called Proximity Processing [ 8 8 ]. It ’ s task was to implement a robust and multilingual system able to analyze/comprehend medical sentences, and to preserve a knowledge of fre...","or French [ 1 0 , 7 2 , 9 4 , 1 13 ], and finally, a proper NLP system called RECIT [ 9 , 1 1 , 1 7 , 1 06 ] has been developed using a method called Proximity Processing [ 8 8 ]. It ’ s task was to implement a robust and multilingual system able to analyze/comprehend medical sentences, and to prese...","[ 8 8 ] | [ 8 8 ] | Proximity Processing
[ 8 8 ]",Format_8 8_Pattern_1 | Format_8 8_Pattern_2 | Format_8 8_Pattern_6,"44, 44, 44"
89,Morin E (1 99 9) Automatic acquisition of semantic relations between terms from technical corpora. In proc.,4,"terms is still a difficulty. PROMETHEE is a system that extracts lexico-syntactic patterns relative to a specific conceptual relation (Morin,1 99 9) [ 8 9 ]. IE systems should work at many levels, from word recognition to discourse analysis at the level of the complete document. An application of th...","terms is still a difficulty. PROMETHEE is a system that extracts lexico-syntactic patterns relative to a specific conceptual relation (Morin,1 99 9) [ 8 9 ]. IE systems should work at many levels, from word recognition to discourse analysis at the level of the complete document. An application of th...","ons between the terms is still a difficulty. PROMETHEE is a system that extracts lexico-syntactic patterns relative to a specific conceptual relation (Morin,1 99 9) [ 8 9 ]. IE systems should work at many levels, from word recognition to discourse analysis at the level of the complete document. An a...","[ 8 9 ] | [ 8 9 ] | (Morin,1 99 9) [ 8 9 ] | It has been suggested that many IE systems can
successfully extract terms from documents, acquiring relations between the terms is still a
difficulty. PROMETHEE is a system that extracts lexico-syntactic patterns relative to a
specific conceptual relation (Morin,1 99 9) [ 8 9 ]",Format_8 9_Pattern_1 | Format_8 9_Pattern_2 | Format_8 9_Pattern_5 | Format_8 9_Pattern_6,"40, 40, 40, 40"
90,"Müller M, Salathé M, Kummervold PE (2 02 0) Covid-twitter-bert: A natural language processing model to",4,have one vector representation for “ bank ” in both the sentences whereas BERT will have different vector representation for “ bank ” . Muller et al. [ 9 0 ] used the BERT model to analyze the tweets on covid-1 9 content. The use of the BERT model in the legal domain was explored by Chalkidis et al....,"or various NLP tasks such as question answering, sentiment analysis, text classification, sentence embedding, interpreting ambiguity in the text etc. [ 2 5 , 3 3 , 9 0 , 1 48 ]. Earlier language-based models examine the text in either of one direction which is used for sentence generation by predict...",have one vector representation for “ bank ” in both the sentences whereas BERT will have different vector representation for “ bank ” . Muller et al. [ 9 0 ] used the BERT model to analyze the tweets on covid-1 9 content. The use of the BERT model in the legal domain was explored by Chalkidis et al....,"[ 9 0 ] | [ 2 5 , 3 3 , 9 0 , 1 48 ] | [ 9 0 ] | Earlier language-based models examine the text in either of one direction which is used for
sentence generation by predicting the next word whereas the BERT model examines the text in
both directions simultaneously for better language understanding. BERT provides contextual
embedding for each word present in the text unlike context-free models (word 2 vec and
Glo Ve). For example, in the sentences “ he is going to the riverbank for a walk ” and “ he is
going to the bank to withdraw some money ” , word 2 vec will have one vector representation for
“ bank ” in both the sentences whereas BERT will have different vector representation for
“ bank ” . Muller et al. [ 9 0 ]",Format_9 0_Pattern_1 | Format_9 0_Pattern_2 | Format_9 0_Pattern_2 | Format_9 0_Pattern_6,"63, 63, 63, 63"
93,Newatia R (2 01 9) https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-,4,"Text Classification [ 1 18 ], Text Summarization [ 1 58 ], Machine Translation [ 7 0 ] and Answer Relations [ 1 50 ] . An article by Newatia (2 01 9) [ 9 3 ] illustrates the general architecture behind any CNN model, and how it can be used in the context of NLP. One can also refer to the work of Wan...","Text Classification [ 1 18 ], Text Summarization [ 1 58 ], Machine Translation [ 7 0 ] and Answer Relations [ 1 50 ] . An article by Newatia (2 01 9) [ 9 3 ] illustrates the general architecture behind any CNN model, and how it can be used in the context of NLP. One can also refer to the work of Wan...","1 35 ], Text Classification [ 1 18 ], Text Summarization [ 1 58 ], Machine Translation [ 7 0 ] and Answer Relations [ 1 50 ] . An article by Newatia (2 01 9) [ 9 3 ] illustrates the general architecture behind any CNN model, and how it can be used in the context of NLP. One can also refer to the wor...",[ 9 3 ] | [ 9 3 ] | (2 01 9) [ 9 3 ] | An article by Newatia (2 01 9) [ 9 3 ],Format_9 3_Pattern_1 | Format_9 3_Pattern_2 | Format_9 3_Pattern_5 | Format_9 3_Pattern_6,"29, 29, 29, 29"
94,"Nhàn NT, Sager N, Lyman M, Tick LJ, Borst F, Su Y (1 98 9) A medical language processor for two indo-",1,"vironment with NLP features [ 8 1 , 1 19 ]. In the first phase, patient records were archived. At later stage the LSP-MLP has been adapted for French [ 1 0 , 7 2 , 9 4 , 1 13 ], and finally, a proper NLP system called RECIT [ 9 , 1 1 , 1 7 , 1 06 ] has been developed using a method called Proximity ...",,,"[ 1 0 , 7 2 , 9 4 , 1 13 ]",Format_9 4_Pattern_2,44
95,"Nießen S, Och FJ, Leusch G, Ney H (2 00 0) An evaluation tool for machine translation: fast evaluation for",4,"llmann et al., 1 99 7) [ 1 38 ], generation string accuracy (Bangalore et al., 2 00 0) [ 8 ], multi-reference word error rate (Nießen et al., 2 00 0) [ 9 5 ], BLEU score (Papineni et al., 2 00 2) [ 1 01 ], NIST score (Doddington, 2 00 2) [ 3 5 ] All these criteria try to approximate human assessment...","llmann et al., 1 99 7) [ 1 38 ], generation string accuracy (Bangalore et al., 2 00 0) [ 8 ], multi-reference word error rate (Nießen et al., 2 00 0) [ 9 5 ], BLEU score (Papineni et al., 2 00 2) [ 1 01 ], NIST score (Doddington, 2 00 2) [ 3 5 ] All these criteria try to approximate human assessment...","dent word error rate (Tillmann et al., 1 99 7) [ 1 38 ], generation string accuracy (Bangalore et al., 2 00 0) [ 8 ], multi-reference word error rate (Nießen et al., 2 00 0) [ 9 5 ], BLEU score (Papineni et al., 2 00 2) [ 1 01 ], NIST score (Doddington, 2 00 2) [ 3 5 ] All these criteria try to appr...","[ 9 5 ] | [ 9 5 ] | (Nießen et al., 2 00 0) [ 9 5 ] | Nießen et al., 2 00 0) [ 9 5 ]",Format_9 5_Pattern_1 | Format_9 5_Pattern_2 | Format_9 5_Pattern_5 | Format_9 5_Pattern_6,"36, 36, 36, 36"
96,"Ochoa, A. (2 01 6). Meet the Pilot: Smart Earpiece Language Translator. https://www.indiegogo.com/",3,"served settings. Link: https://www.ncbi.nlm.nih.gov/pubmed/2 82 69 89 5?dopt=Abstract e) Meet the Pilot, world ’ s first language translating earbuds [ 9 6 ] The world ’ s first smart earpiece Pilot will soon be transcribed over 1 5 languages. According to Spring wise, Waverly Labs ’ Pilot can alrea...","served settings. Link: https://www.ncbi.nlm.nih.gov/pubmed/2 82 69 89 5?dopt=Abstract e) Meet the Pilot, world ’ s first language translating earbuds [ 9 6 ] The world ’ s first smart earpiece Pilot will soon be transcribed over 1 5 languages. According to Spring wise, Waverly Labs ’ Pilot can alrea...",ure-bi d) Using Natural Language Processing and Network Analysis to Develop a Conceptual Framework for Medication Therapy Management Research [ 9 7 ] Natural Language Processing and Network Analysis to Develop a Conceptual Framework for Medication Therapy Management Research describes a theory deriv...,"[ 9 6 ] | [ 9 6 ] | Natural Language Processing and Network Analysis to Develop a Conceptual Framework for
Medication Therapy Management Research describes a theory derivation process that is used
to develop a conceptual framework for medication therapy management (MTM) research. The
MTM service model and chronic care model are selected as parent theories. Review article
abstracts target medication therapy management in chronic disease care that were retrieved
from Ovid Medline (2 00 0 – 2 01 6). Unique concepts in each abstract are extracted using Meta
Map and their pair-wise co-occurrence are determined. Then the information is used to
construct a network graph of concept co-occurrence that is further analyzed to identify content
for the new conceptual model. 1 42 abstracts are analyzed. Medication adherence is the most
studied drug therapy problem and co-occurred with concepts related to patient-centered
interventions targeting self-management. The enhanced model consists of 6 5 concepts clus-
tered into 1 4 constructs. The framework requires additional refinement and evaluation to
determine its relevance and applicability across a broad audience including underserved
settings.
Link: https://www.ncbi.nlm.nih.gov/pubmed/2 82 69 89 5?dopt=Abstract
e)
Meet the Pilot, world ’ s first language translating earbuds [ 9 6 ]",Format_9 6_Pattern_1 | Format_9 6_Pattern_2 | Format_9 6_Pattern_6,"47, 47, 47"
97,"Ogallo, W., & Kanter, A. S. (2 01 7). Using natural language processing and network analysis to develop a",3,sing-future-bi d) Using Natural Language Processing and Network Analysis to Develop a Conceptual Framework for Medication Therapy Management Research [ 9 7 ] Natural Language Processing and Network Analysis to Develop a Conceptual Framework for Medication Therapy Management Research describes a theo...,sing-future-bi d) Using Natural Language Processing and Network Analysis to Develop a Conceptual Framework for Medication Therapy Management Research [ 9 7 ] Natural Language Processing and Network Analysis to Develop a Conceptual Framework for Medication Therapy Management Research describes a theo...,correct. Link: https://www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/ c) Future of BI in Natural Language Processing [ 1 40 ] Several companies in BI spaces are trying to get with the trend and trying hard to ensure that data becomes more friendly and easily accessible. But s...,"[ 9 7 ] | [ 9 7 ] | Several companies in BI spaces are trying to get with the trend and trying hard to ensure that
data becomes more friendly and easily accessible. But still there is a long way for this.BI will
also make it easier to access as GUI is not needed. Because nowadays the queries are made by
text or voice command on smartphones.one of the most common examples is Google might
tell you today what tomorrow ’ s weather will be. But soon enough, we will be able to ask our
personal data chatbot about customer sentiment today, and how we feel about their brand next
week; all while walking down the street. Today, NLP tends to be based on turning natural
language into machine language. But with time the technology matures – especially the AI
component – the computer will get better at “ understanding ” the query and start to deliver
answers rather than search results. Initially, the data chatbot will probably ask the question
‘ how have revenues changed over the last three-quarters? ’ and then return pages of data for
you to analyze. But once it learns the semantic relations and inferences of the question, it will
be able to automatically perform the filtering and formulation necessary to provide an
intelligible answer, rather than simply showing you data.
Link: http://www.smartdatacollective.com/eran-levy/4 89 41 0/here-s-why-natural-language-
processing-future-bi
d)
Using Natural Language Processing and Network Analysis to Develop a Conceptual
Framework for Medication Therapy Management Research [ 9 7 ]",Format_9 7_Pattern_1 | Format_9 7_Pattern_2 | Format_9 7_Pattern_6,"47, 47, 47"
98,"Otter DW, Medina JR, Kalita JK (2 02 0) A survey of the usages of deep learning for natural language",4,"del which compresses memories for long-range sequence learning, may be helpful for the readers. One may also refer to the recent work by Otter et al. [ 9 8 ] on uses of Deep Learning for NLP, and Fig. 3 A walkthrough of recent developments in NLP 3 72 2 Multimedia Tools and Applications (2 02 3) 8 2...","del which compresses memories for long-range sequence learning, may be helpful for the readers. One may also refer to the recent work by Otter et al. [ 9 8 ] on uses of Deep Learning for NLP, and Fig. 3 A walkthrough of recent developments in NLP 3 72 2 Multimedia Tools and Applications (2 02 3) 8 2...","Transformer-XL (XL as extra- long) which enables learning dependencies beyond a fixed length of words. Further the work of Rae et al. [ 1 04 ] on the Compressive Transformer, an attentive sequence model which compresses memories for long-range sequence learning, may be helpful for the readers. One m...","[ 9 8 ] | [ 9 8 ] | Compressive Transformer, an attentive sequence model which
compresses memories for long-range sequence learning, may be helpful for the readers. One
may also refer to the recent work by Otter et al. [ 9 8 ] | [ 1 5 , 3 2 , 6 3 , 9 8 , 1 33 , 1 51 ]",Format_9 8_Pattern_1 | Format_9 8_Pattern_2 | Format_9 8_Pattern_6 | Format_9 8_Pattern_2,"31, 31, 31, 73"
99,"Ouyang Y, Li W, Li S, Lu Q (2 01 1) Applying regression models to query-focused multi-document",3,also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani and Maybury 1 99 9 [ 7 4 ]; Fattah and Ren 2 00 9 [ 4 3 ]; Riedhammer et al...,also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani and Maybury 1 99 9 [ 7 4 ]; Fattah and Ren 2 00 9 [ 4 3 ]; Riedhammer et al...,4 3 ]).Summaries can also be of two types: generic or query-focused (Gong and Liu 2 00 1 [ 5 0 ]; Dunlavy et al. 2 00 7 [ 3 7 ]; Wan 2 00 8 [ 1 44 ]; Ouyang et al. 2 01 1 [ 9 9 ]).Summarization task can be either supervised or unsupervised (Mani and Maybury 1 99 9 [ 7 4 ]; Fattah and Ren 2 00 9 [ 4 ...,[ 9 9 ] | [ 9 9 ] | Ouyang et al. 2 01 1 [ 9 9 ],Format_9 9_Pattern_1 | Format_9 9_Pattern_2 | Format_9 9_Pattern_6,"42, 42, 42"
100,"Palmer M, Gildea D, Kingsbury P (2 00 5) The proposition bank: an annotated corpus of semantic roles.",0,NOT CITED,,,,,
101,"Papineni K, Roukos S, Ward T, Zhu WJ (2 00 2) BLEU: a method for automatic evaluation of machine",0,NOT CITED,,,,,
102,"Peng Y, Chi J (2 01 9) Unsupervised cross-media retrieval using domain adaptation with scene graph. IEEE",0,NOT CITED,,,,,
103,Porter MF (1 98 0) An algorithm for suffix stripping. Program 1 4(3):1 30 – 1 37,0,NOT CITED,,,,,
104,"Rae JW, Potapenko A, Jayakumar SM, Lillicrap TP, (2 01 9) Compressive transformers for long-range",0,NOT CITED,,,,,
105,"Ranjan P, Basu HVSSA (2 00 3) Part of speech tagging and local word grouping techniques for natural",0,NOT CITED,,,,,
106,"Rassinoux AM, Baud RH, Scherrer JR (1 99 2) Conceptual graphs model extension for knowledge",0,NOT CITED,,,,,
107,"Rassinoux AM, Michel PA, Juge C, Baud R, Scherrer JR (1 99 4) Natural language processing of medical",0,NOT CITED,,,,,
108,"Rassinoux AM, Juge C, Michel PA, Baud RH, Lemaitre D, Jean FC, Scherrer JR (1 99 5) Analysis of",0,NOT CITED,,,,,
109,Rennie J (2 00 0) ifile: An application of machine learning to e-mail filtering. In Proc. KDD 2 00 0 Workshop,0,NOT CITED,,,,,
110,"Riedhammer K, Favre B, Hakkani-Tür D (2 01 0) Long story short – global unsupervised models for",0,NOT CITED,,,,,
111,"Ritter A, Clark S, Etzioni O (2 01 1) Named entity recognition in tweets: an experimental study. In",0,NOT CITED,,,,,
112,"Rospocher M, van Erp M, Vossen P, Fokkens A, Aldabe I, Rigau G, Soroa A, Ploeger T, Bogaard T(2 01 6)",0,NOT CITED,,,,,
113,"Sager N, Lyman M, Tick LJ, Borst F, Nhan NT, Revillard C, … Scherrer JR (1 98 9) Adapting a medical",0,NOT CITED,,,,,
114,"Sager N, Lyman M, Nhan NT, Tick LJ (1 99 5) Medical language processing: applications to patient data",0,NOT CITED,,,,,
115,"Sahami M, Dumais S, Heckerman D, Horvitz E (1 99 8) A Bayesian approach to filtering junk e-mail. In",0,NOT CITED,,,,,
116,"Sakkis G, Androutsopoulos I, Paliouras G, Karkaletsis V, Spyropoulos CD, Stamatopoulos P (2 00 1)",0,NOT CITED,,,,,
117,"Sakkis G, Androutsopoulos I, Paliouras G et al (2 00 3) A memory-based approach to anti-spam filtering for",0,NOT CITED,,,,,
118,"Santoro A, Faulkner R, Raposo D, Rae J, Chrzanowski M, Weber T, ..., Lillicrap T (2 01 8) Relational",0,NOT CITED,,,,,
119,"Scherrer JR, Revillard C, Borst F, Berthoud M, Lovis C (1 99 4) Medical office automation integrated into",0,NOT CITED,,,,,
120,"Seal D, Roy UK, Basak R (2 02 0) Sentence-level emotion detection from text based on semantic rules. In:",0,NOT CITED,,,,,
121,"Sentiraama Corpus by Gangula Rama Rohit Reddy, Radhika Mamidi. Language Technologies Research",0,NOT CITED,,,,,
122,"Sha F, Pereira F (2 00 3) Shallow parsing with conditional random fields. In proceedings of the 2 00 3",0,NOT CITED,,,,,
123,"Sharifirad S, Matwin S, (2 01 9) When a tweet is actually sexist. A more comprehensive classification of",0,NOT CITED,,,,,
124,"Sharma S, Srinivas PYKL, Balabantaray RC (2 01 6) Emotion Detection using Online Machine Learning",0,NOT CITED,,,,,
125,Shemtov H (1 99 7) Ambiguity management in natural language generation. Stanford University,0,NOT CITED,,,,,
126,"Small SL, Cortell GW, Tanenhaus MK (1 98 8) Lexical Ambiguity Resolutions. Morgan Kauffman, San",0,NOT CITED,,,,,
127,"Socher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C (2 01 3) Recursive deep models for",0,NOT CITED,,,,,
128,"Sonnhammer EL, Eddy SR, Birney E, Bateman A, Durbin R (1 99 8) Pfam: multiple sequence alignments",0,NOT CITED,,,,,
129,Srihari S (2 01 0) Machine Learning: Generative and Discriminative Models. http://www.cedar.buffalo.edu/,0,NOT CITED,,,,,
130,"Sun X, Morency LP, Okanohara D, Tsujii JI (2 00 8) Modeling latent-dynamic in shallow parsing: a latent",0,NOT CITED,,,,,
131,"Sundheim BM, Chinchor NA (1 99 3) Survey of the message understanding conferences. In proceedings of",0,NOT CITED,,,,,
132,"Sutskever I, Vinyals O, Le QV (2 01 4) Sequence to sequence learning with neural networks. In Advances",0,NOT CITED,,,,,
133,"Sworna ZT, Mousavi Z, Babar MA (2 02 2) NLP methods in host-based intrusion detection Systems: A",0,NOT CITED,,,,,
134,"Systems RAVN (2 01 7) ""RAVN Systems Launch the ACE Powered GDPR Robot - Artificial Intelligence",0,NOT CITED,,,,,
135,"Tan KL, Lee CP, Anbananthen KSM, Lim KM (2 02 2) Ro BERTa-LSTM: A hybrid model for sentiment",0,NOT CITED,,,,,
136,"Tapaswi N, Jain S (2 01 2) Treebank based deep grammar acquisition and part-of-speech tagging for",0,NOT CITED,,,,,
137,Thomas C (2 01 9) https://towardsdatascience.com/recurrent-neural-networks-and-natural-language-,0,NOT CITED,,,,,
138,"Tillmann C, Vogel S, Ney H, Zubiaga A, Sawaf H (1 99 7) Accelerated DP based search for statistical",0,NOT CITED,,,,,
139,"Umber A, Bajwa I (2 01 1) “ Minimizing ambiguity in natural language software requirements specification,",0,NOT CITED,,,,,
141,"Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł , Polosukhin I, (2 01 7)",0,NOT CITED,,,,,
142,"Wahlster W, Kobsa A (1 98 9) User models in dialog systems. In user models in dialog systems (pp. 4 – 3 4).",0,NOT CITED,,,,,
143,"Walton D (1 99 6) A pragmatic synthesis. In: fallacies arising from ambiguity. Applied logic series, vol 1.",0,NOT CITED,,,,,
144,Wan X (2 00 8) Using only cross-document relationships for both generic and topic-focused multi-docu-,0,NOT CITED,,,,,
145,"Wang W, Gang J, 2 01 8 Application of convolutional neural network in natural language processing. In",0,NOT CITED,,,,,
146,"Wang D, Zhu S, Li T, Gong Y (2 00 9) Multi-document summarization using sentence-based topic models.",0,NOT CITED,,,,,
147,"Wang D, Zhu S, Li T, Chi Y, Gong Y (2 01 1) Integrating document clustering and multidocument",0,NOT CITED,,,,,
148,"Wang Z, Ng P, Ma X, Nallapati R, Xiang B (2 01 9) Multi-passage bert: A globally normalized bert model",0,NOT CITED,,,,,
149,"Wen Z, Peng Y (2 02 0) Multi-level knowledge injecting for visual commonsense reasoning. IEEE",0,NOT CITED,,,,,
150,"Wiese G, Weissenborn D, Neves M (2 01 7) Neural domain adaptation for biomedical question answering.",0,NOT CITED,,,,,
151,"Wong A, Plasek JM, Montecalvo SP, Zhou L (2 01 8) Natural language processing and its implications for",0,NOT CITED,,,,,
152,Woods WA (1 97 8) Semantics and quantification in natural language question answering. Adv Comput 1 7:,0,NOT CITED,,,,,
153,Xia T (2 02 0) A constant time complexity spam detection algorithm for boosting throughput on rule-based,0,NOT CITED,,,,,
154,"Xie P, Xing E (2 01 7) A constituent-centric neural architecture for reading comprehension. In proceedings",0,NOT CITED,,,,,
155,"Yan X, Ye Y, Mao Y, Yu H (2 01 9) Shared-private information bottleneck method for cross-modal",0,NOT CITED,,,,,
156,"Yi J, Nasukawa T, Bunescu R, Niblack W (2 00 3) Sentiment analyzer: extracting sentiments about a given",0,NOT CITED,,,,,
157,"Young SJ, Chase LL (1 99 8) Speech recognition evaluation: a review of the US CSR and LVCSR",0,NOT CITED,,,,,
158,"Yu S, et al. (2 01 8) ""A multi-stage memory augmented neural network for machine reading comprehen-",0,NOT CITED,,,,,
159,"Zajic DM, Dorr BJ, Lin J (2 00 8) Single-document and multi-document summarization techniques for",0,NOT CITED,,,,,
160,"Zeroual I, Lakhouaja A, Belahbib R (2 01 7) Towards a standard part of speech tagset for the Arabic",0,NOT CITED,,,,,
