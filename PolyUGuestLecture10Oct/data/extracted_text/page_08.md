# Page 08

3720                                             Multimedia Tools and Applications (2023) 82:3713–3744


b) Components and Levels of Representation

The process of language generation involves the following interweaved tasks. Content selec-
tion: Information should be selected and included in the set. Depending on how this informa-
tion is parsed into representational units, parts of the units may have to be removed while some
others may be added by default. Textual Organization: The information must be textually
organized according to the grammar, it must be ordered both sequentially and in terms of
linguistic relations like modifications. Linguistic Resources: To support the information’s
realization, linguistic resources must be chosen. In the end these resources will come down
to choices of particular words, idioms, syntactic constructs etc. Realization: The selected and
organized resources must be realized as an actual text or voice output.

c) Application or Speaker

This is only for maintaining the model of the situation. Here the speaker just initiates the
process doesn’t take part in the language generation. It stores the history, structures the content
that is potentially relevant and deploys a representation of what it knows. All these forms the
situation, while selecting subset of propositions that speaker has. The only requirement is the
speaker must make sense of the situation [91].


3 NLP: Then and now

In the late 1940s the term NLP wasn’t in existence, but the work regarding machine translation
(MT) had started. In fact, Research in this period was not completely localized. Russian and
English were the dominant languages for MT (Andreev,1967) [4]. In fact, MT/NLP research
almost died in 1966 according to the ALPAC report, which concluded that MT is going
nowhere. But later, some MT production systems were providing output to their customers
(Hutchins, 1986) [60]. By this time, work on the use of computers for literary and linguistic
studies had also started. As early as 1960, signature work influenced by AI began, with the
BASEBALL Q-A systems (Green et al., 1961) [51]. LUNAR (Woods,1978) [152] and
Winograd SHRDLU were natural successors of these systems, but they were seen as
stepped-up sophistication, in terms of their linguistic and their task processing capabilities.
There was a widespread belief that progress could only be made on the two sides, one is
ARPA Speech Understanding Research (SUR) project (Lea, 1980) and other in some major
system developments projects building database front ends. The front-end projects (Hendrix
et al., 1978) [55] were intended to go beyond LUNAR in interfacing the large databases. In
early 1980s computational grammar theory became a very active area of research linked with
logics for meaning and knowledge’s ability to deal with the user’s beliefs and intentions and
with functions like emphasis and themes.
    By the end of the decade the powerful general purpose sentence processors like SRI’s Core
Language Engine (Alshawi,1992) [2] and Discourse Representation Theory (Kamp and
Reyle,1993) [62] offered a means of tackling more extended discourse within the
grammatico-logical framework. This period was one of the growing communities. Practical
resources, grammars, and tools and parsers became available (for example: Alvey Natural
Language Tools) (Briscoe et al., 1987) [18]. The (D)ARPA speech recognition and message
understanding (information extraction) conferences were not only for the tasks they addressed
